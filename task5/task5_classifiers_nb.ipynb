{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка необходимых библиотек"
      ],
      "metadata": {
        "id": "e8aE79ml0ey_"
      },
      "id": "e8aE79ml0ey_"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JAlbV140EGM",
        "outputId": "52d6bdd9-5980-479a-ffca-c54be792e2ef"
      },
      "id": "6JAlbV140EGM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем датасеты"
      ],
      "metadata": {
        "id": "sbTgc1Er0x9C"
      },
      "id": "sbTgc1Er0x9C"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_news = load_dataset(\"ag_news\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbTAJCrR005B",
        "outputId": "27c5091b-75f6-4044-d2d1-f46859f110af"
      },
      "id": "bbTAJCrR005B",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# всего 4 класса, по 25% каждого в dataset['train']\n",
        "# сформирую датасет из 8000 примеров, по 2000 примеров на каждый класс\n",
        "\n",
        "count0, count1, count2, count3 = 0, 0, 0, 0\n",
        "dataset_short = []\n",
        "for i in range(len(dataset_news['train'])):\n",
        "  if dataset_news['train'][i]['label'] == 0 and count0 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 1 and count1 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 2 and count2 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 2})\n",
        "    count2 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 3 and count3 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 3})\n",
        "    count3 += 1\n",
        "len(dataset_short)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLgBaNJE1WRR",
        "outputId": "e79b9fc7-fb11-41b1-e0bc-cee7583fc925"
      },
      "id": "rLgBaNJE1WRR",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(dataset_short)\n",
        "dataset_news = {'train': dataset_short[:9600], 'test': dataset_short[9600:]}"
      ],
      "metadata": {
        "id": "-J84rVRE1rXZ"
      },
      "id": "-J84rVRE1rXZ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выводим объём выборок\n",
        "print('Объём train:', len(dataset_news['train']))\n",
        "print('Объём test:', len(dataset_news['test']))\n",
        "\n",
        "train_texts_news = [s['news'] for s in dataset_news['train']]\n",
        "test_texts_news = [s['news'] for s in dataset_news['test']]\n",
        "\n",
        "train_labels_news = [s['label'] for s in dataset_news['train']]\n",
        "test_labels_news = [s['label'] for s in dataset_news['test']]\n",
        "\n",
        "# ВЫводим примеры\n",
        "print(train_texts_news[0], train_labels_news[0])\n",
        "print(test_texts_news[0], test_labels_news[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DQ_0vTM2CeW",
        "outputId": "192ecec3-81d8-432e-a1ce-1f33dabc20ee"
      },
      "id": "9DQ_0vTM2CeW",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Объём train: 9600\n",
            "Объём test: 2400\n",
            "New Computer Is Created Just for Teenagers This isn't your typical, humdrum, slate-colored computer. Not only is the PC known as the hip-e almost all white, but its screen and keyboard are framed in fuzzy pink fur. Or a leopard skin design. Or a graffiti-themed pattern. 3\n",
            "Dim prospects for peace With public attention focused elsewhere, the Bush administration is letting Middle East peace hopes disappear. Israelis rightly complain that peace requires a partner, a role that Yasser Arafat has been pathetically incapable of filling so far. 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# второй датасет\n",
        "dataset_imdb = load_dataset(\"imdb\")\n",
        "\n",
        "count0, count1 = 0, 0\n",
        "dataset_short_imdb = []\n",
        "\n",
        "for i in range(len(dataset_imdb['train'])):\n",
        "  if dataset_imdb['train'][i]['label'] == 0 and count0 < 6000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_imdb['train'][i]['label'] == 1 and count1 < 6000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "len(dataset_short_imdb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoE93PjQ2WgD",
        "outputId": "b64c084a-adfd-47d4-cc6b-9080deb323ea"
      },
      "id": "xoE93PjQ2WgD",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(dataset_short_imdb)\n",
        "dataset_imdb = {'train': dataset_short_imdb[:9600], 'test': dataset_short_imdb[9600:]}"
      ],
      "metadata": {
        "id": "ebqLP9tr2g_s"
      },
      "id": "ebqLP9tr2g_s",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выводим объём выборок\n",
        "print('Объём train:', len(dataset_imdb['train']))\n",
        "print('Объём test:', len(dataset_imdb['test']))\n",
        "\n",
        "train_texts_imdb = [s['text'] for s in dataset_imdb['train']]\n",
        "test_texts_imdb = [s['text'] for s in dataset_imdb['test']]\n",
        "\n",
        "train_labels_imdb = [s['label'] for s in dataset_imdb['train']]\n",
        "test_labels_imdb = [s['label'] for s in dataset_imdb['test']]\n",
        "\n",
        "print(train_texts_imdb[0])\n",
        "print(train_labels_imdb[0])\n",
        "print()\n",
        "print(test_texts_imdb[0])\n",
        "print(test_labels_imdb[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iGl4rfj206i",
        "outputId": "84d1f2ec-34d8-44bf-c8e6-95b033bd9380"
      },
      "id": "-iGl4rfj206i",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Объём train: 9600\n",
            "Объём test: 2400\n",
            "I may very well be one of the few who really stuck to this film. I also saw this movie when it came out, and I agree with the last post that Up The Acedemy was way ahead of its' time. The humor in the film itself is pure MAD magazine. I don't see why MAD stand behind this feature. It was also one of the few films of the early 80's to have a killer accompanying soundtrack with the punk and new wave bands that were emerging from L.A. at the time. I own the soundtrack and I play it constantly to this day. What can I say? There are definitely worst movies out there. I don't consider Porky's to be as funny as Up The Academy, there are some really good laughs throughout the film, and the jokes fall on either stereotypes or getting laid. Hey, nobody said this was going to be The Maltese Falcon.\n",
            "1\n",
            "\n",
            "A journey of discovery, this film follows the lives of one family living in a sleepy, island town in British Columbia. Languorous and dreamy, the inhabitants are satisfied to allow life to go on around them until a young, fresh-faced teacher, with new ideas arrives and brings with her life from the mainland. Slowly, their indolent state is awakened, the father (and principal of the local school) looks for excitement, the mother for stability, the oldest daughter for love, and the youngest for power. While not an incredible or ground-breaking piece of cinema, the movie is quietly enjoyable and good for a tired night when the wind is blowing. Unfortunately, I doubt anyone outside of Canada will find it easily accessible.\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# news example\n",
        "news_example = train_texts_news[0]\n",
        "print(news_example)\n",
        "\n",
        "# imdb example\n",
        "imdb_example = train_texts_imdb[0]\n",
        "print(imdb_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1aRXp7V3OoS",
        "outputId": "73bed163-896a-43c9-e8e6-468873f3aec3"
      },
      "id": "G1aRXp7V3OoS",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Computer Is Created Just for Teenagers This isn't your typical, humdrum, slate-colored computer. Not only is the PC known as the hip-e almost all white, but its screen and keyboard are framed in fuzzy pink fur. Or a leopard skin design. Or a graffiti-themed pattern.\n",
            "I may very well be one of the few who really stuck to this film. I also saw this movie when it came out, and I agree with the last post that Up The Acedemy was way ahead of its' time. The humor in the film itself is pure MAD magazine. I don't see why MAD stand behind this feature. It was also one of the few films of the early 80's to have a killer accompanying soundtrack with the punk and new wave bands that were emerging from L.A. at the time. I own the soundtrack and I play it constantly to this day. What can I say? There are definitely worst movies out there. I don't consider Porky's to be as funny as Up The Academy, there are some really good laughs throughout the film, and the jokes fall on either stereotypes or getting laid. Hey, nobody said this was going to be The Maltese Falcon.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функции предобработки"
      ],
      "metadata": {
        "id": "XUNSy5NI3WgH"
      },
      "id": "XUNSy5NI3WgH"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from sklearn.metrics import f1_score\n",
        "stop_words = list(set(stopwords.words(\"english\")))\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gopQXv1d3Zpe",
        "outputId": "60fd0b3d-b619-4753-edd4-f9267ed1c613"
      },
      "id": "gopQXv1d3Zpe",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def base_preprocessing(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Zа-яА-Я ]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "xvOmvI3o4B8V"
      },
      "id": "xvOmvI3o4B8V",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_special(text):\n",
        "  tokens = base_preprocessing(text)\n",
        "  tokens = [word for word in tokens if word not in stop_words]  # убираем стоп-слова\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "_ybZJgt83hza"
      },
      "id": "_ybZJgt83hza",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(text):\n",
        "  tokens = stopwords_special(text)\n",
        "\n",
        "  stemmer = nltk.PorterStemmer()  # инициализируем стеммер\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens]  # перебираем токены и применяем алгоритм стемминга\n",
        "  return stemmed_tokens"
      ],
      "metadata": {
        "id": "LrxXHbFb3nGZ"
      },
      "id": "LrxXHbFb3nGZ",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatisation(text):\n",
        "  tokens = stopwords_special(text)\n",
        "\n",
        "  lemma = nltk.WordNetLemmatizer()\n",
        "  lemma_tokens = [lemma.lemmatize(token) for token in tokens]\n",
        "  return lemma_tokens"
      ],
      "metadata": {
        "id": "twhYRrDs5Jd6"
      },
      "id": "twhYRrDs5Jd6",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизаторы"
      ],
      "metadata": {
        "id": "mRmtXMNS6kbn"
      },
      "id": "mRmtXMNS6kbn"
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(train_texts, test_texts, method, edit):\n",
        "  if method == \"count\":\n",
        "    vectorizer = CountVectorizer()\n",
        "  else:\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # edit = 1 (stopwords_specoal), 2 (stemming), 3 (lemmatisation)\n",
        "  if edit == 1:\n",
        "    train_texts = [' '.join(stopwords_special(text)) for text in train_texts]\n",
        "    test_texts = [' '.join(stopwords_special(text)) for text in test_texts]\n",
        "  elif edit == 2:\n",
        "    train_texts = [' '.join(stemming(text)) for text in train_texts]\n",
        "    test_texts = [' '.join(stemming(text)) for text in test_texts]\n",
        "  elif edit == 3:\n",
        "    train_texts = [' '.join(lemmatisation(text)) for text in train_texts]\n",
        "    test_texts = [' '.join(lemmatisation(text)) for text in test_texts]\n",
        "\n",
        "  train_text = vectorizer.fit_transform(train_texts)\n",
        "  test_text = vectorizer.transform(test_texts)\n",
        "\n",
        "  return train_text, test_text"
      ],
      "metadata": {
        "id": "s881EEK_6mHH"
      },
      "id": "s881EEK_6mHH",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение классификаторов"
      ],
      "metadata": {
        "id": "UvG0u_WH7oSM"
      },
      "id": "UvG0u_WH7oSM"
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for alpha in [0.01, 0.1, 1, 10]:\n",
        "  for f_p in [True, False]:\n",
        "    for d in ['news', 'imdb']:\n",
        "      for m in ['count', 'tfidf']:\n",
        "        for edit in [1, 2, 3]:\n",
        "          if d == 'news':\n",
        "            train_texts = train_texts_news\n",
        "            test_texts = test_texts_news\n",
        "            train_labels = train_labels_news\n",
        "            test_labels = test_labels_news\n",
        "          else:\n",
        "            train_texts = train_texts_imdb\n",
        "            test_texts = test_texts_imdb\n",
        "            train_labels = train_labels_imdb\n",
        "            test_labels = test_labels_imdb\n",
        "\n",
        "\n",
        "          train_text, test_text = vectorize(train_texts, test_texts, m, edit)\n",
        "\n",
        "          nb_clf = MultinomialNB(alpha=alpha, fit_prior=f_p)\n",
        "          nb_clf.fit(train_text, train_labels)\n",
        "          nb_pred = nb_clf.predict(test_text)\n",
        "          if edit == 1:\n",
        "            results.append([alpha, f_p, d, m, 'stopwords_special', f1_score(test_labels, nb_pred, average='macro')])\n",
        "          elif edit == 2:\n",
        "            results.append([alpha, f_p, d, m, 'stemming', f1_score(test_labels, nb_pred, average='macro')])\n",
        "          elif edit == 3:\n",
        "            results.append([alpha, f_p, d, m, 'lemmatisation', f1_score(test_labels, nb_pred, average='macro')])\n",
        "\n",
        "          if len(results) % 10 == 0:\n",
        "            print(len(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f5dql5A7qIh",
        "outputId": "5e3559de-697e-4613-f1a6-2a7d8da8faa6"
      },
      "id": "6f5dql5A7qIh",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBhXJixpMRDP",
        "outputId": "ed893037-f1fb-4bcc-f52a-c6b00235cf2d"
      },
      "id": "kBhXJixpMRDP",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.01, True, 'news', 'count', 'stopwords_special', 0.8780675206735631],\n",
              " [0.01, True, 'news', 'count', 'stemming', 0.8843334796324799],\n",
              " [0.01, True, 'news', 'count', 'lemmatisation', 0.8793422603990133],\n",
              " [0.01, True, 'news', 'tfidf', 'stopwords_special', 0.8833754494048445],\n",
              " [0.01, True, 'news', 'tfidf', 'stemming', 0.8830513968196638],\n",
              " [0.01, True, 'news', 'tfidf', 'lemmatisation', 0.8796559113898796],\n",
              " [0.01, True, 'imdb', 'count', 'stopwords_special', 0.8465361157929489],\n",
              " [0.01, True, 'imdb', 'count', 'stemming', 0.8477517045813007],\n",
              " [0.01, True, 'imdb', 'count', 'lemmatisation', 0.8435805184054459],\n",
              " [0.01, True, 'imdb', 'tfidf', 'stopwords_special', 0.8499623864038137]]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results, columns=[\"alpha (сглаживание)\", \"fit_prior (апр.вероятности)\", \"Dataset\", \"Vectorizer\", \"Preprocess\", \"F1-score\"])\n",
        "df_results.to_excel(\"results_nb.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "lKZUD7QWMZX9"
      },
      "id": "lKZUD7QWMZX9",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "results_lsa = []\n",
        "for k in [10, 50, 100]:\n",
        "  for alpha in [0.01, 0.1, 1, 10]:\n",
        "    for f_p in [True, False]:\n",
        "      for d in ['news', 'imdb']:\n",
        "        for m in ['count', 'tfidf']:\n",
        "          for edit in [1, 2, 3]:\n",
        "            if d == 'news':\n",
        "              train_texts = train_texts_news\n",
        "              test_texts = test_texts_news\n",
        "              train_labels = train_labels_news\n",
        "              test_labels = test_labels_news\n",
        "            else:\n",
        "              train_texts = train_texts_imdb\n",
        "              test_texts = test_texts_imdb\n",
        "              train_labels = train_labels_imdb\n",
        "              test_labels = test_labels_imdb\n",
        "\n",
        "            train_text, test_text = vectorize(train_texts, test_texts, m, edit)\n",
        "            svd = TruncatedSVD(n_components=k, random_state=42)\n",
        "            lsa = make_pipeline(svd)\n",
        "            train_lsa = lsa.fit_transform(train_text)\n",
        "            test_lsa = lsa.transform(test_text)\n",
        "\n",
        "            train_lsa = np.clip(train_lsa, 0, None)\n",
        "            test_lsa = np.clip(test_lsa, 0, None)\n",
        "\n",
        "            nb_clf = MultinomialNB(alpha=alpha, fit_prior=f_p)\n",
        "            nb_clf.fit(train_lsa, train_labels)\n",
        "            nb_pred = nb_clf.predict(test_lsa)\n",
        "            if edit == 1:\n",
        "              results_lsa.append([k, alpha, f_p, d, m, 'stopwords_special', f1_score(test_labels, nb_pred, average='macro')])\n",
        "            elif edit == 2:\n",
        "              results_lsa.append([k, alpha, f_p, d, m, 'stemming', f1_score(test_labels, nb_pred, average='macro')])\n",
        "            elif edit == 3:\n",
        "              results_lsa.append([k, alpha, f_p, d, m, 'lemmatisation', f1_score(test_labels, nb_pred, average='macro')])\n",
        "\n",
        "            if len(results_lsa) % 10 == 0:\n",
        "              print(len(results_lsa))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lXVXFIrP2j9",
        "outputId": "7a5d885b-4f38-4f00-9a68-8333961fcbfb"
      },
      "id": "9lXVXFIrP2j9",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_lsa[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBdQfw0go4Xw",
        "outputId": "8afb8579-a49b-4cd5-b5fc-007681bd8cfc"
      },
      "id": "yBdQfw0go4Xw",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[10, 0.01, True, 'news', 'count', 'stopwords_special', 0.6223650693556702],\n",
              " [10, 0.01, True, 'news', 'count', 'stemming', 0.6539763789563652],\n",
              " [10, 0.01, True, 'news', 'count', 'lemmatisation', 0.6678196769883877],\n",
              " [10, 0.01, True, 'news', 'tfidf', 'stopwords_special', 0.6153712559371532],\n",
              " [10, 0.01, True, 'news', 'tfidf', 'stemming', 0.7379325116235838],\n",
              " [10, 0.01, True, 'news', 'tfidf', 'lemmatisation', 0.6896944164305527],\n",
              " [10, 0.01, True, 'imdb', 'count', 'stopwords_special', 0.6653998785702964],\n",
              " [10, 0.01, True, 'imdb', 'count', 'stemming', 0.6611087364715744],\n",
              " [10, 0.01, True, 'imdb', 'count', 'lemmatisation', 0.6493417078539014],\n",
              " [10, 0.01, True, 'imdb', 'tfidf', 'stopwords_special', 0.744809736833298]]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results1 = pd.DataFrame(results_lsa, columns=['k', \"alpha (сглаживание)\", \"fit_prior (апр.вероятности)\", \"Dataset\", \"Vectorizer\", \"Preprocess\", \"F1-score\"])\n",
        "df_results1.to_excel(\"results_lsa_nb.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "y5oRi7_xpDky"
      },
      "id": "y5oRi7_xpDky",
      "execution_count": 38,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}