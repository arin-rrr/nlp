{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка необходимых библиотек"
      ],
      "metadata": {
        "id": "e8aE79ml0ey_"
      },
      "id": "e8aE79ml0ey_"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "6JAlbV140EGM"
      },
      "id": "6JAlbV140EGM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем датасеты"
      ],
      "metadata": {
        "id": "sbTgc1Er0x9C"
      },
      "id": "sbTgc1Er0x9C"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_news = load_dataset(\"ag_news\")"
      ],
      "metadata": {
        "id": "bbTAJCrR005B"
      },
      "id": "bbTAJCrR005B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# всего 4 класса, по 25% каждого в dataset['train']\n",
        "# сформирую датасет из 8000 примеров, по 2000 примеров на каждый класс\n",
        "\n",
        "count0, count1, count2, count3 = 0, 0, 0, 0\n",
        "dataset_short = []\n",
        "for i in range(len(dataset_news['train'])):\n",
        "  if dataset_news['train'][i]['label'] == 0 and count0 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 1 and count1 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 2 and count2 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 2})\n",
        "    count2 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 3 and count3 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 3})\n",
        "    count3 += 1\n",
        "len(dataset_short)"
      ],
      "metadata": {
        "id": "rLgBaNJE1WRR"
      },
      "id": "rLgBaNJE1WRR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(dataset_short)\n",
        "dataset_news = {'train': dataset_short[:9600], 'test': dataset_short[9600:]}"
      ],
      "metadata": {
        "id": "-J84rVRE1rXZ"
      },
      "id": "-J84rVRE1rXZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выводим объём выборок\n",
        "print('Объём train:', len(dataset_news['train']))\n",
        "print('Объём test:', len(dataset_news['test']))\n",
        "\n",
        "train_texts_news = [s['news'] for s in dataset_news['train']]\n",
        "test_texts_news = [s['news'] for s in dataset_news['test']]\n",
        "\n",
        "train_labels_news = [s['label'] for s in dataset_news['train']]\n",
        "test_labels_news = [s['label'] for s in dataset_news['test']]\n",
        "\n",
        "# ВЫводим примеры\n",
        "print(train_texts_news[0], train_labels_news[0])\n",
        "print(test_texts_news[0], test_labels_news[0])"
      ],
      "metadata": {
        "id": "9DQ_0vTM2CeW"
      },
      "id": "9DQ_0vTM2CeW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# второй датасет\n",
        "dataset_imdb = load_dataset(\"imdb\")\n",
        "\n",
        "count0, count1 = 0, 0\n",
        "dataset_short_imdb = []\n",
        "\n",
        "for i in range(len(dataset_imdb['train'])):\n",
        "  if dataset_imdb['train'][i]['label'] == 0 and count0 < 6000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_imdb['train'][i]['label'] == 1 and count1 < 6000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "len(dataset_short_imdb)"
      ],
      "metadata": {
        "id": "xoE93PjQ2WgD"
      },
      "id": "xoE93PjQ2WgD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(dataset_short_imdb)\n",
        "dataset_imdb = {'train': dataset_short_imdb[:9600], 'test': dataset_short_imdb[9600:]}"
      ],
      "metadata": {
        "id": "ebqLP9tr2g_s"
      },
      "id": "ebqLP9tr2g_s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выводим объём выборок\n",
        "print('Объём train:', len(dataset_imdb['train']))\n",
        "print('Объём test:', len(dataset_imdb['test']))\n",
        "\n",
        "train_texts_imdb = [s['text'] for s in dataset_imdb['train']]\n",
        "test_texts_imdb = [s['text'] for s in dataset_imdb['test']]\n",
        "\n",
        "train_labels_imdb = [s['label'] for s in dataset_imdb['train']]\n",
        "test_labels_imdb = [s['label'] for s in dataset_imdb['test']]\n",
        "\n",
        "print(train_texts_imdb[0])\n",
        "print(train_labels_imdb[0])\n",
        "print()\n",
        "print(test_texts_imdb[0])\n",
        "print(test_labels_imdb[0])"
      ],
      "metadata": {
        "id": "-iGl4rfj206i"
      },
      "id": "-iGl4rfj206i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# news example\n",
        "news_example = train_texts_news[0]\n",
        "print(news_example)\n",
        "\n",
        "# imdb example\n",
        "imdb_example = train_texts_imdb[0]\n",
        "print(imdb_example)"
      ],
      "metadata": {
        "id": "G1aRXp7V3OoS"
      },
      "id": "G1aRXp7V3OoS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функции предобработки"
      ],
      "metadata": {
        "id": "XUNSy5NI3WgH"
      },
      "id": "XUNSy5NI3WgH"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from sklearn.metrics import f1_score\n",
        "stop_words = list(set(stopwords.words(\"english\")))\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "gopQXv1d3Zpe"
      },
      "id": "gopQXv1d3Zpe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def base_preprocessing(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Zа-яА-Я ]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "xvOmvI3o4B8V"
      },
      "id": "xvOmvI3o4B8V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_special(text):\n",
        "  tokens = base_preprocessing(text)\n",
        "  tokens = [word for word in tokens if word not in stop_words]  # убираем стоп-слова\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "_ybZJgt83hza"
      },
      "id": "_ybZJgt83hza",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(text):\n",
        "  tokens = stopwords_special(text)\n",
        "\n",
        "  stemmer = nltk.PorterStemmer()  # инициализируем стеммер\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens]  # перебираем токены и применяем алгоритм стемминга\n",
        "  return stemmed_tokens"
      ],
      "metadata": {
        "id": "LrxXHbFb3nGZ"
      },
      "id": "LrxXHbFb3nGZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatisation(text):\n",
        "  tokens = stopwords_special(text)\n",
        "\n",
        "  lemma = nltk.WordNetLemmatizer()\n",
        "  lemma_tokens = [lemma.lemmatize(token) for token in tokens]\n",
        "  return lemma_tokens"
      ],
      "metadata": {
        "id": "twhYRrDs5Jd6"
      },
      "id": "twhYRrDs5Jd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизаторы"
      ],
      "metadata": {
        "id": "mRmtXMNS6kbn"
      },
      "id": "mRmtXMNS6kbn"
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(train_texts, test_texts, method, edit):\n",
        "  if method == \"count\":\n",
        "    vectorizer = CountVectorizer()\n",
        "  else:\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # edit = 1 (stopwords_specoal), 2 (stemming), 3 (lemmatisation)\n",
        "  if edit == 1:\n",
        "    train_texts = [' '.join(stopwords_special(text)) for text in train_texts]\n",
        "    test_texts = [' '.join(stopwords_special(text)) for text in test_texts]\n",
        "  elif edit == 2:\n",
        "    train_texts = [' '.join(stemming(text)) for text in train_texts]\n",
        "    test_texts = [' '.join(stemming(text)) for text in test_texts]\n",
        "  elif edit == 3:\n",
        "    train_texts = [' '.join(lemmatisation(text)) for text in train_texts]\n",
        "    test_texts = [' '.join(lemmatisation(text)) for text in test_texts]\n",
        "\n",
        "  train_text = vectorizer.fit_transform(train_texts)\n",
        "  test_text = vectorizer.transform(test_texts)\n",
        "\n",
        "  return train_text, test_text"
      ],
      "metadata": {
        "id": "s881EEK_6mHH"
      },
      "id": "s881EEK_6mHH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение классификаторов"
      ],
      "metadata": {
        "id": "UvG0u_WH7oSM"
      },
      "id": "UvG0u_WH7oSM"
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for alpha in [0.01, 0.1, 1, 10]:\n",
        "  for f_p in [True, False]:\n",
        "    for d in ['news', 'imdb']:\n",
        "      for m in ['count', 'tfidf']:\n",
        "        for edit in [1, 2, 3]:\n",
        "          if d == 'news':\n",
        "            train_texts = train_texts_news\n",
        "            test_texts = test_texts_news\n",
        "            train_labels = train_labels_news\n",
        "            test_labels = test_labels_news\n",
        "          else:\n",
        "            train_texts = train_texts_imdb\n",
        "            test_texts = test_texts_imdb\n",
        "            train_labels = train_labels_imdb\n",
        "            test_labels = test_labels_imdb\n",
        "\n",
        "\n",
        "          train_text, test_text = vectorize(train_texts, test_texts, m, edit)\n",
        "\n",
        "          nb_clf = MultinomialNB(alpha=alpha, fit_prior=f_p)\n",
        "          nb_clf.fit(train_text, train_labels)\n",
        "          nb_pred = nb_clf.predict(test_text)\n",
        "          if edit == 1:\n",
        "            results.append([alpha, f_p, d, m, 'stopwords_special', f1_score(test_labels, nb_pred, average='macro')])\n",
        "          elif edit == 2:\n",
        "            results.append([alpha, f_p, d, m, 'stemming', f1_score(test_labels, nb_pred, average='macro')])\n",
        "          elif edit == 3:\n",
        "            results.append([alpha, f_p, d, m, 'lemmatisation', f1_score(test_labels, nb_pred, average='macro')])\n",
        "\n",
        "          if len(results) % 10 == 0:\n",
        "            print(len(results))"
      ],
      "metadata": {
        "id": "6f5dql5A7qIh"
      },
      "id": "6f5dql5A7qIh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[:10]"
      ],
      "metadata": {
        "id": "kBhXJixpMRDP"
      },
      "id": "kBhXJixpMRDP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results, columns=[\"alpha (сглаживание)\", \"fit_prior (апр.вероятности)\", \"Dataset\", \"Vectorizer\", \"Preprocess\", \"F1-score\"])\n",
        "df_results.to_excel(\"results_nb.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "lKZUD7QWMZX9"
      },
      "id": "lKZUD7QWMZX9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "results_lsa = []\n",
        "for k in [10, 50, 100]:\n",
        "  for alpha in [0.01, 0.1, 1, 10]:\n",
        "    for f_p in [True, False]:\n",
        "      for d in ['news', 'imdb']:\n",
        "        for m in ['count', 'tfidf']:\n",
        "          for edit in [1, 2, 3]:\n",
        "            if d == 'news':\n",
        "              train_texts = train_texts_news\n",
        "              test_texts = test_texts_news\n",
        "              train_labels = train_labels_news\n",
        "              test_labels = test_labels_news\n",
        "            else:\n",
        "              train_texts = train_texts_imdb\n",
        "              test_texts = test_texts_imdb\n",
        "              train_labels = train_labels_imdb\n",
        "              test_labels = test_labels_imdb\n",
        "\n",
        "            train_text, test_text = vectorize(train_texts, test_texts, m, edit)\n",
        "            svd = TruncatedSVD(n_components=k, random_state=42)\n",
        "            lsa = make_pipeline(svd)\n",
        "            train_lsa = lsa.fit_transform(train_text)\n",
        "            test_lsa = lsa.transform(test_text)\n",
        "\n",
        "            train_lsa = np.clip(train_lsa, 0, None)\n",
        "            test_lsa = np.clip(test_lsa, 0, None)\n",
        "\n",
        "            nb_clf = MultinomialNB(alpha=alpha, fit_prior=f_p)\n",
        "            nb_clf.fit(train_lsa, train_labels)\n",
        "            nb_pred = nb_clf.predict(test_lsa)\n",
        "            if edit == 1:\n",
        "              results_lsa.append([k, alpha, f_p, d, m, 'stopwords_special', f1_score(test_labels, nb_pred, average='macro')])\n",
        "            elif edit == 2:\n",
        "              results_lsa.append([k, alpha, f_p, d, m, 'stemming', f1_score(test_labels, nb_pred, average='macro')])\n",
        "            elif edit == 3:\n",
        "              results_lsa.append([k, alpha, f_p, d, m, 'lemmatisation', f1_score(test_labels, nb_pred, average='macro')])\n",
        "\n",
        "            if len(results_lsa) % 10 == 0:\n",
        "              print(len(results_lsa))"
      ],
      "metadata": {
        "id": "9lXVXFIrP2j9"
      },
      "id": "9lXVXFIrP2j9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_lsa[:10]"
      ],
      "metadata": {
        "id": "yBdQfw0go4Xw"
      },
      "id": "yBdQfw0go4Xw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results1 = pd.DataFrame(results_lsa, columns=['k', \"alpha (сглаживание)\", \"fit_prior (апр.вероятности)\", \"Dataset\", \"Vectorizer\", \"Preprocess\", \"F1-score\"])\n",
        "df_results1.to_excel(\"results_lsa_nb.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "y5oRi7_xpDky"
      },
      "id": "y5oRi7_xpDky",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}