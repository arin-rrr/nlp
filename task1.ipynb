{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88728bd-c2c7-428c-921d-76e2dd7bd707",
   "metadata": {},
   "source": [
    "**Задание 1.1**\n",
    "Написать самим или разобрать код, запустить, посмотреть на 3х-5ти текстах как работает, найти различия, которые возникают в результате различных подходов к предобработке текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee94d9c-ba05-4de3-94fe-66a617b0a177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\arina\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c02d1-460e-4834-a6f0-f1f44998dac1",
   "metadata": {},
   "source": [
    "- punkt - для токенезации (разбиения текста на слова и фразы)\n",
    "- wordnet - для лемматизации (приведения слов к начальной формн)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a86342-e800-4e6c-b731-7f2ef03e42ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\arina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\arina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44be25-06f0-4a33-be9c-a52e7b212df6",
   "metadata": {},
   "source": [
    "Датасет **AG News** - Датасет с новостными заголовками и краткими описаниями, разбитыми на 4 категории (World,Sports, Business, Sci/Tech). Объём – 120 тыс. в обучающем наборе и 7600 в тестовом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5dc765-e5df-440d-802a-5ec323c3aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a35827-9599-4a88-b518-ef1ec9da6fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объем train dataset:  120000\n",
      "{'text': 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.', 'label': 2}\n",
      "Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n"
     ]
    }
   ],
   "source": [
    "# Берём второй пример из обучающей выборки\n",
    "first_example = ds[\"train\"][1]\n",
    "first_text = first_example[\"text\"]\n",
    "print('Объем train dataset: ', len(ds['train']))\n",
    "print(first_example)\n",
    "print(first_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6626509-e300-4373-a0a0-76465d199c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Carlyle',\n",
       " 'Looks',\n",
       " 'Toward',\n",
       " 'Commercial',\n",
       " 'Aerospace',\n",
       " '(',\n",
       " 'Reuters',\n",
       " ')',\n",
       " 'Reuters',\n",
       " '-',\n",
       " 'Private',\n",
       " 'investment',\n",
       " 'firm',\n",
       " 'Carlyle',\n",
       " 'Group',\n",
       " ',',\n",
       " '\\\\which',\n",
       " 'has',\n",
       " 'a',\n",
       " 'reputation',\n",
       " 'for',\n",
       " 'making',\n",
       " 'well-timed',\n",
       " 'and',\n",
       " 'occasionally\\\\controversial',\n",
       " 'plays',\n",
       " 'in',\n",
       " 'the',\n",
       " 'defense',\n",
       " 'industry',\n",
       " ',',\n",
       " 'has',\n",
       " 'quietly',\n",
       " 'placed\\\\its',\n",
       " 'bets',\n",
       " 'on',\n",
       " 'another',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'market',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# с этим текстом произведем токенезацию\n",
    "token = nltk.word_tokenize(first_text)\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c155b4-2327-4f7c-9e2b-ca408b517dac",
   "metadata": {},
   "source": [
    "Теперь проведём предобработку со:\n",
    "- стемминг - процесс удаления окончаний и суффиксов из слова, чтобы получить его основу (или \"корень\"): бегущий - бег, качественный - качеств\n",
    "- лемматизация - процесс приведения слова к его нормальной словарной форме: бегущий - бежать, качественный - качественный, играя - играть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21222472-cbec-4f85-8b51-c56c737dce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция предобработки со стеммингом\n",
    "def stemming(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление знаков препинания\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Применение стемминга\n",
    "    # Алгоритм использует набор правил, которые последовательно применяются к слову, чтобы удалить суффиксы. Эти правила основаны на структуре слова и его окончаниях.\n",
    "    stemmer = nltk.PorterStemmer()  # инициализируем стеммер\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # перебираем токены и применяем алгоритм стемминга\n",
    "    \n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a009df46-9bef-4575-beea-a0e8614f84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция предобработки с лемматизацией\n",
    "def lemmatization(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление знаков препинания\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Применение лемматизации\n",
    "    # WordNet - алгоритм ищет слово в базе данных WordNet и возвращает его базовую форму (лемму)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8502b26-df64-40ad-8745-16c141a4cfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Изначальный текст: Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "\n",
      "Стемминг: ['carlyl', 'look', 'toward', 'commerci', 'aerospac', 'reuter', 'reuter', 'privat', 'invest', 'firm', 'carlyl', 'groupwhich', 'ha', 'a', 'reput', 'for', 'make', 'welltim', 'and', 'occasionallycontroversi', 'play', 'in', 'the', 'defens', 'industri', 'ha', 'quietli', 'placedit', 'bet', 'on', 'anoth', 'part', 'of', 'the', 'market']\n",
      "\n",
      "Лемматизация: ['carlyle', 'look', 'toward', 'commercial', 'aerospace', 'reuters', 'reuters', 'private', 'investment', 'firm', 'carlyle', 'groupwhich', 'ha', 'a', 'reputation', 'for', 'making', 'welltimed', 'and', 'occasionallycontroversial', 'play', 'in', 'the', 'defense', 'industry', 'ha', 'quietly', 'placedits', 'bet', 'on', 'another', 'part', 'of', 'the', 'market']\n"
     ]
    }
   ],
   "source": [
    "# применяем оба алгоритма к примеру\n",
    "st_first = stemming(first_text)\n",
    "lemma_first = lemmatization(first_text)\n",
    "\n",
    "print('Изначальный текст:', first_text)\n",
    "print()\n",
    "print('Стемминг:', st_first)\n",
    "print()\n",
    "print('Лемматизация:', lemma_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6073bdd9-cfbe-4547-93a0-43f258afe26f",
   "metadata": {},
   "source": [
    "**Различия**:\n",
    "- **Стемминг** возвращает основу слова, которая не всегда является реальным словом, **лемматизация** возвращает словарную форму (лемму).\n",
    "- Стэмминг менее точен, чем лемматизация, так как не учитывает часть речи слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "640e5765-1500-4c36-a8d0-00c5824d2207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Новый пример: Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "\n",
      "Стемминг: ['oil', 'and', 'economi', 'cloud', 'stock', 'outlook', 'reuter', 'reuter', 'soar', 'crude', 'price', 'plu', 'worriesabout', 'the', 'economi', 'and', 'the', 'outlook', 'for', 'earn', 'are', 'expect', 'tohang', 'over', 'the', 'stock', 'market', 'next', 'week', 'dure', 'the', 'depth', 'of', 'thesumm', 'doldrum']\n",
      "Время предобработки со стеммингом: 0.0009729862213134766\n",
      "\n",
      "Лемматизация: ['oil', 'and', 'economy', 'cloud', 'stock', 'outlook', 'reuters', 'reuters', 'soaring', 'crude', 'price', 'plus', 'worriesabout', 'the', 'economy', 'and', 'the', 'outlook', 'for', 'earnings', 'are', 'expected', 'tohang', 'over', 'the', 'stock', 'market', 'next', 'week', 'during', 'the', 'depth', 'of', 'thesummer', 'doldrums']\n",
      "Время предобработки с лемматизацией: 0.0004432201385498047\n"
     ]
    }
   ],
   "source": [
    "# оценим скорость выполнения стемминга и лемматизации на новом примере\n",
    "import time\n",
    "\n",
    "second_text = ds[\"train\"][2][\"text\"]\n",
    "print('Новый пример:', second_text)\n",
    "print()\n",
    "\n",
    "begin_stemming = time.time()\n",
    "st_second = stemming(second_text)\n",
    "end_stemming = time.time()\n",
    "\n",
    "begin_lemma = time.time()\n",
    "lemma_second = lemmatization(second_text)\n",
    "end_lemma = time.time()\n",
    "\n",
    "print('Стемминг:', st_second)\n",
    "print('Время предобработки со стеммингом:', end_stemming - begin_stemming)\n",
    "print()\n",
    "print('Лемматизация:', lemma_second)\n",
    "print('Время предобработки с лемматизацией:', end_lemma-begin_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5f32a9-84e4-4e62-ad25-3f8705bec30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Новый пример: Making Your Insurer Pay If Hurricane Charley blows your house down, how can you make your insurance company pay?\n",
      "\n",
      "Стемминг: ['make', 'your', 'insur', 'pay', 'if', 'hurrican', 'charley', 'blow', 'your', 'hous', 'down', 'how', 'can', 'you', 'make', 'your', 'insur', 'compani', 'pay']\n",
      "Время предобработки со стеммингом: 0.0004591941833496094\n",
      "\n",
      "Лемматизация: ['making', 'your', 'insurer', 'pay', 'if', 'hurricane', 'charley', 'blow', 'your', 'house', 'down', 'how', 'can', 'you', 'make', 'your', 'insurance', 'company', 'pay']\n",
      "Время предобработки с лемматизацией: 0.0003147125244140625\n"
     ]
    }
   ],
   "source": [
    "# ещё один пример\n",
    "third_text = ds[\"train\"][50][\"text\"]\n",
    "print('Новый пример:', third_text)\n",
    "print()\n",
    "\n",
    "begin_stemming1 = time.time()\n",
    "st_third = stemming(third_text)\n",
    "end_stemming1 = time.time()\n",
    "\n",
    "begin_lemma1 = time.time()\n",
    "lemma_third = lemmatization(third_text)\n",
    "end_lemma1 = time.time()\n",
    "\n",
    "print('Стемминг:', st_third)\n",
    "print('Время предобработки со стеммингом:', end_stemming1 - begin_stemming1)\n",
    "print()\n",
    "print('Лемматизация:', lemma_third)\n",
    "print('Время предобработки с лемматизацией:', end_lemma1 - begin_lemma1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cbe13-fdc0-409d-9be3-2ac91224df9b",
   "metadata": {},
   "source": [
    "**Задание 1.2**: Вывести на экран и посмотреть, какая разница между результатами стемминга и лемматизации, показать конкретные случаи различия. Если требуется, можно вывести на экран больше текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "021731c4-bff2-4853-a58d-3b18a2be4812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "\n",
      "Стемминг: ['carlyl', 'look', 'toward', 'commerci', 'aerospac', 'reuter', 'reuter', 'privat', 'invest', 'firm', 'carlyl', 'groupwhich', 'ha', 'a', 'reput', 'for', 'make', 'welltim', 'and', 'occasionallycontroversi', 'play', 'in', 'the', 'defens', 'industri', 'ha', 'quietli', 'placedit', 'bet', 'on', 'anoth', 'part', 'of', 'the', 'market']\n",
      "\n",
      "Лематизация: ['carlyle', 'look', 'toward', 'commercial', 'aerospace', 'reuters', 'reuters', 'private', 'investment', 'firm', 'carlyle', 'groupwhich', 'ha', 'a', 'reputation', 'for', 'making', 'welltimed', 'and', 'occasionallycontroversial', 'play', 'in', 'the', 'defense', 'industry', 'ha', 'quietly', 'placedits', 'bet', 'on', 'another', 'part', 'of', 'the', 'market']\n"
     ]
    }
   ],
   "source": [
    "# первый пример\n",
    "print('Исходный текст:', first_text)\n",
    "print()\n",
    "print('Стемминг:', st_first)\n",
    "print()\n",
    "print('Лематизация:', lemma_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b8001-4822-4728-aa09-2c81382c3836",
   "metadata": {},
   "source": [
    "**Конкретные примеры различия:**\n",
    "- Carlyle - carlyl - carlyle (стемминг убрал суффикс)\n",
    "- Commercial - commerci - commercial (стемминг просто оставил \"корень\": коммерческий - коммерч - коммерческий)\n",
    "- investment - invest - investment (инвестиции - инвестировать - инвестиции, стемминг, оставив корень, из существительного сделал глагол)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac9142c8-0b78-4094-956b-2f84236a35f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "\n",
      "Стемминг: ['oil', 'and', 'economi', 'cloud', 'stock', 'outlook', 'reuter', 'reuter', 'soar', 'crude', 'price', 'plu', 'worriesabout', 'the', 'economi', 'and', 'the', 'outlook', 'for', 'earn', 'are', 'expect', 'tohang', 'over', 'the', 'stock', 'market', 'next', 'week', 'dure', 'the', 'depth', 'of', 'thesumm', 'doldrum']\n",
      "\n",
      "Лематизация: ['oil', 'and', 'economy', 'cloud', 'stock', 'outlook', 'reuters', 'reuters', 'soaring', 'crude', 'price', 'plus', 'worriesabout', 'the', 'economy', 'and', 'the', 'outlook', 'for', 'earnings', 'are', 'expected', 'tohang', 'over', 'the', 'stock', 'market', 'next', 'week', 'during', 'the', 'depth', 'of', 'thesummer', 'doldrums']\n"
     ]
    }
   ],
   "source": [
    "# второй пример\n",
    "print('Исходный текст:', second_text)\n",
    "print()\n",
    "print('Стемминг:', st_second)\n",
    "print()\n",
    "print('Лематизация:', lemma_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a917a0f-c47e-4bf4-bf6e-5e662bf53f65",
   "metadata": {},
   "source": [
    "**Примеры:**\n",
    "- в обоих случаях убираем множественное число: Stocks - stock - stock\n",
    "- Economy - economi - economy (не поняла, почему в стемминге i вместо e)\n",
    "- Soaring - soar - soaring (стемминг не учитывает часть речи)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e228b4d-d17a-4428-adba-c34529847a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Making Your Insurer Pay If Hurricane Charley blows your house down, how can you make your insurance company pay?\n",
      "\n",
      "Стемминг: ['make', 'your', 'insur', 'pay', 'if', 'hurrican', 'charley', 'blow', 'your', 'hous', 'down', 'how', 'can', 'you', 'make', 'your', 'insur', 'compani', 'pay']\n",
      "\n",
      "Лематизация: ['making', 'your', 'insurer', 'pay', 'if', 'hurricane', 'charley', 'blow', 'your', 'house', 'down', 'how', 'can', 'you', 'make', 'your', 'insurance', 'company', 'pay']\n"
     ]
    }
   ],
   "source": [
    "# третий пример\n",
    "print('Исходный текст:', third_text)\n",
    "print()\n",
    "print('Стемминг:', st_third)\n",
    "print()\n",
    "print('Лематизация:', lemma_third)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4d5f6-442d-4801-a04f-3b5d1dca8edb",
   "metadata": {},
   "source": [
    "**Примеры:**\n",
    "- Making - make - making (стемминг не учитывает часть речи)\n",
    "- Insurer - insur - insurer (во время стемминга убрано окончание)\n",
    "- Insurance - insur - insurance\n",
    "- blows - blow - blow (убираем -s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32de547-16d2-49e1-b326-95ffed4472c1",
   "metadata": {},
   "source": [
    "**Задание 1.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffdee70e-c95f-4e73-a0b3-09ee5740a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Comets, Asteroids and Planets around a Nearby Star (SPACE.com) SPACE.com - A nearby star thought to harbor comets and asteroids now appears to be home to planets, too. The presumed worlds are smaller than Jupiter and could be as tiny as Pluto, new observations suggest.\n",
      "\n",
      "Стандартная токенезация: ['Comets', ',', 'Asteroids', 'and', 'Planets', 'around', 'a', 'Nearby', 'Star', '(', 'SPACE.com', ')', 'SPACE.com', '-', 'A', 'nearby', 'star', 'thought', 'to', 'harbor', 'comets', 'and', 'asteroids', 'now', 'appears', 'to', 'be', 'home', 'to', 'planets', ',', 'too', '.', 'The', 'presumed', 'worlds', 'are', 'smaller', 'than', 'Jupiter', 'and', 'could', 'be', 'as', 'tiny', 'as', 'Pluto', ',', 'new', 'observations', 'suggest', '.']\n"
     ]
    }
   ],
   "source": [
    "# рассмотрим пример со знаками препинания ,.-()\n",
    "example = ds['train'][100]['text']\n",
    "\n",
    "token_example = nltk.word_tokenize(example)\n",
    "print('Исходный текст:', example)\n",
    "print()\n",
    "print('Стандартная токенезация:', token_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8661474c-97da-4484-9af8-15d6c6013b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стемминг: ['comet', 'asteroid', 'and', 'planet', 'around', 'a', 'nearbi', 'star', 'spacecom', 'spacecom', 'a', 'nearbi', 'star', 'thought', 'to', 'harbor', 'comet', 'and', 'asteroid', 'now', 'appear', 'to', 'be', 'home', 'to', 'planet', 'too', 'the', 'presum', 'world', 'are', 'smaller', 'than', 'jupit', 'and', 'could', 'be', 'as', 'tini', 'as', 'pluto', 'new', 'observ', 'suggest']\n",
      "\n",
      "Лемматизация: ['comet', 'asteroid', 'and', 'planet', 'around', 'a', 'nearby', 'star', 'spacecom', 'spacecom', 'a', 'nearby', 'star', 'thought', 'to', 'harbor', 'comet', 'and', 'asteroid', 'now', 'appears', 'to', 'be', 'home', 'to', 'planet', 'too', 'the', 'presumed', 'world', 'are', 'smaller', 'than', 'jupiter', 'and', 'could', 'be', 'a', 'tiny', 'a', 'pluto', 'new', 'observation', 'suggest']\n"
     ]
    }
   ],
   "source": [
    "# предобработка через стемминг и лемматизацию (перед токенезацией удаляем знаки препинания)\n",
    "st_example = stemming(example)\n",
    "lemma_example = lemmatization(example)\n",
    "print('Стемминг:', st_example)\n",
    "print()\n",
    "print('Лемматизация:', lemma_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec4e8a-5080-49bb-aa17-6d0c8888f2b2",
   "metadata": {},
   "source": [
    "**При лемматизации as заменяется на a, но -s на конце слова не мн.число или окончание глагола**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d84e59-aa45-4738-b9f2-9e232ab6cffd",
   "metadata": {},
   "source": [
    "Теперь посмотрим на результат обработки, если убираем знаки препинания после токенезации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06649035-48e0-49ff-8ac4-b054db937756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Comets, Asteroids and Planets around a Nearby Star (SPACE.com) SPACE.com - A nearby star thought to harbor comets and asteroids now appears to be home to planets, too. The presumed worlds are smaller than Jupiter and could be as tiny as Pluto, new observations suggest.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ac7de75-3416-41fb-9b6f-3ff45be03b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_marks = [\n",
    "    '.', ',', ';', ':', '!', '?', '-', '–', '—', '(', ')', \n",
    "    '[', ']', '{', '}', '\"', \"'\", '`', '‘', '’', '“', '”', \n",
    "    '/', '\\\\', '|', '@', '#', '$', '%', '^', '&', '*', '_', \n",
    "    '=', '+', '<', '>', '~'\n",
    "]\n",
    "\n",
    "def remove_punct_marks(token):\n",
    "    new_tokens = []\n",
    "    for i in token:\n",
    "        if i not in punctuation_marks:\n",
    "            new_tokens.append(i)\n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "def steming_1(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "\n",
    "    # Токенизация\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Теперь удаляем знаки препинания\n",
    "    tokens = remove_punct_marks(tokens)\n",
    "\n",
    "    # Применение стемминга\n",
    "    # Алгоритм использует набор правил, которые последовательно применяются к слову, чтобы удалить суффиксы. Эти правила основаны на структуре слова и его окончаниях.\n",
    "    stemmer = nltk.PorterStemmer()  # инициализируем стеммер\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # перебираем токены и применяем алгоритм стемминга\n",
    "    \n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "502b092b-c9ad-4655-9017-c52da2655e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comet', 'asteroid', 'and', 'planet', 'around', 'a', 'nearby', 'star', 'space.com', 'space.com', 'a', 'nearby', 'star', 'thought', 'to', 'harbor', 'comet', 'and', 'asteroid', 'now', 'appears', 'to', 'be', 'home', 'to', 'planet', 'too', 'the', 'presumed', 'world', 'are', 'smaller', 'than', 'jupiter', 'and', 'could', 'be', 'a', 'tiny', 'a', 'pluto', 'new', 'observation', 'suggest']\n"
     ]
    }
   ],
   "source": [
    "def lemmatization_1(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "\n",
    "    # Токенизация\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Удаление знаков препинания\n",
    "    tokens = remove_punct_marks(tokens)\n",
    "    \n",
    "    # Применение лемматизации\n",
    "    # WordNet - алгоритм ищет слово в базе данных WordNet и возвращает его базовую форму (лемму)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "print(lemmatization_1(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27fac75b-6acd-4277-ac4b-de7c2001b091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удаление знаков препинания -> Токенезация\n",
      "Стемминг: ['comet', 'asteroid', 'and', 'planet', 'around', 'a', 'nearbi', 'star', 'spacecom', 'spacecom', 'a', 'nearbi', 'star', 'thought', 'to', 'harbor', 'comet', 'and', 'asteroid', 'now', 'appear', 'to', 'be', 'home', 'to', 'planet', 'too', 'the', 'presum', 'world', 'are', 'smaller', 'than', 'jupit', 'and', 'could', 'be', 'as', 'tini', 'as', 'pluto', 'new', 'observ', 'suggest']\n",
      "\n",
      "Токенезация -> Удаление знаков препинания\n",
      "Стемминг: ['comet', 'asteroid', 'and', 'planet', 'around', 'a', 'nearbi', 'star', 'space.com', 'space.com', 'a', 'nearbi', 'star', 'thought', 'to', 'harbor', 'comet', 'and', 'asteroid', 'now', 'appear', 'to', 'be', 'home', 'to', 'planet', 'too', 'the', 'presum', 'world', 'are', 'smaller', 'than', 'jupit', 'and', 'could', 'be', 'as', 'tini', 'as', 'pluto', 'new', 'observ', 'suggest']\n"
     ]
    }
   ],
   "source": [
    "print('Удаление знаков препинания -> Токенезация')\n",
    "print('Стемминг:', st_example)\n",
    "print()\n",
    "print('Токенезация -> Удаление знаков препинания')\n",
    "print('Стемминг:', steming_1(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df1a3496-050b-4ff3-bc77-d1c47059d11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удаление знаков препинания -> Токенезация\n",
      "Лемматизация: ['comet', 'asteroid', 'and', 'planet', 'around', 'a', 'nearby', 'star', 'spacecom', 'spacecom', 'a', 'nearby', 'star', 'thought', 'to', 'harbor', 'comet', 'and', 'asteroid', 'now', 'appears', 'to', 'be', 'home', 'to', 'planet', 'too', 'the', 'presumed', 'world', 'are', 'smaller', 'than', 'jupiter', 'and', 'could', 'be', 'a', 'tiny', 'a', 'pluto', 'new', 'observation', 'suggest']\n",
      "\n",
      "Токенезация -> Удаление знаков препинания\n",
      "Лемматизация: ['comet', 'asteroid', 'and', 'planet', 'around', 'a', 'nearby', 'star', 'space.com', 'space.com', 'a', 'nearby', 'star', 'thought', 'to', 'harbor', 'comet', 'and', 'asteroid', 'now', 'appears', 'to', 'be', 'home', 'to', 'planet', 'too', 'the', 'presumed', 'world', 'are', 'smaller', 'than', 'jupiter', 'and', 'could', 'be', 'a', 'tiny', 'a', 'pluto', 'new', 'observation', 'suggest']\n"
     ]
    }
   ],
   "source": [
    "print('Удаление знаков препинания -> Токенезация')\n",
    "print('Лемматизация:', lemma_example)\n",
    "print()\n",
    "print('Токенезация -> Удаление знаков препинания')\n",
    "print('Лемматизация:', lemmatization_1(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab2576-a4bc-4adc-832f-1763b0509314",
   "metadata": {},
   "source": [
    "**Задание 1.5. Добавить фильтрацию стоп слов и вывести результирующий список слов для примеров на двух датасетах.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce440921-f2d6-4bc8-9aeb-c9ea44f9f03d",
   "metadata": {},
   "source": [
    "**Стоп-слова** — это распространенные слова, которые не несут важной смысловой нагрузки в тексте. Например, в английском это \"the\", \"is\", \"in\", \"and\". Они часто удаляются перед анализом текста, чтобы уменьшить объем данных и улучшить качество модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0880f08-6436-4d14-9034-6e7d84da4b44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"should've\",\n",
       " 'doing',\n",
       " 'ourselves',\n",
       " 'more',\n",
       " 'no',\n",
       " 'than',\n",
       " 'themselves',\n",
       " \"that'll\",\n",
       " 'very',\n",
       " 'mustn',\n",
       " 'aren',\n",
       " 'both',\n",
       " 'o',\n",
       " \"hadn't\",\n",
       " 'herself',\n",
       " 'in',\n",
       " 's',\n",
       " 'over',\n",
       " 'shan',\n",
       " 'you',\n",
       " 'out',\n",
       " 'hasn',\n",
       " 'above',\n",
       " 'here',\n",
       " 'for',\n",
       " \"she's\",\n",
       " 'am',\n",
       " 'd',\n",
       " 'ain',\n",
       " 'same',\n",
       " 'were',\n",
       " \"hasn't\",\n",
       " 'any',\n",
       " 'have',\n",
       " 'each',\n",
       " \"don't\",\n",
       " 'doesn',\n",
       " 'me',\n",
       " 'won',\n",
       " 'from',\n",
       " 'do',\n",
       " \"weren't\",\n",
       " 'we',\n",
       " 'was',\n",
       " 'most',\n",
       " \"shouldn't\",\n",
       " 'which',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'whom',\n",
       " 'yourselves',\n",
       " 'she',\n",
       " 'had',\n",
       " 'after',\n",
       " 'all',\n",
       " 'just',\n",
       " 'll',\n",
       " 'own',\n",
       " \"didn't\",\n",
       " 'them',\n",
       " 'been',\n",
       " 'an',\n",
       " 'shouldn',\n",
       " 'up',\n",
       " 'yourself',\n",
       " 'under',\n",
       " 'his',\n",
       " 'does',\n",
       " 'between',\n",
       " 'down',\n",
       " 'further',\n",
       " 'be',\n",
       " \"aren't\",\n",
       " 'such',\n",
       " \"you've\",\n",
       " 'the',\n",
       " 'haven',\n",
       " 'i',\n",
       " 't',\n",
       " 'few',\n",
       " 'her',\n",
       " 'through',\n",
       " 'or',\n",
       " \"won't\",\n",
       " 'having',\n",
       " 'myself',\n",
       " 'ours',\n",
       " 'being',\n",
       " 'how',\n",
       " 'wasn',\n",
       " \"you'll\",\n",
       " 'then',\n",
       " 'off',\n",
       " \"mightn't\",\n",
       " 'who',\n",
       " 'did',\n",
       " 'not',\n",
       " 'your',\n",
       " 'can',\n",
       " 'nor',\n",
       " \"it's\",\n",
       " \"wouldn't\",\n",
       " 'before',\n",
       " 'because',\n",
       " 'by',\n",
       " 'those',\n",
       " 'on',\n",
       " 'there',\n",
       " 'are',\n",
       " 'our',\n",
       " 'has',\n",
       " 'y',\n",
       " 'didn',\n",
       " 'hadn',\n",
       " 'and',\n",
       " 'below',\n",
       " 'where',\n",
       " \"wasn't\",\n",
       " \"you're\",\n",
       " 'to',\n",
       " 'a',\n",
       " 'as',\n",
       " 'that',\n",
       " \"couldn't\",\n",
       " 'should',\n",
       " 'so',\n",
       " 'wouldn',\n",
       " 'ma',\n",
       " 'they',\n",
       " 'he',\n",
       " 'while',\n",
       " 'about',\n",
       " 'hers',\n",
       " 've',\n",
       " \"mustn't\",\n",
       " 'some',\n",
       " 'during',\n",
       " 'again',\n",
       " 'their',\n",
       " 'these',\n",
       " 'yours',\n",
       " 'himself',\n",
       " 'when',\n",
       " 'don',\n",
       " \"doesn't\",\n",
       " 'why',\n",
       " 'only',\n",
       " 'of',\n",
       " 'needn',\n",
       " \"shan't\",\n",
       " 're',\n",
       " 'with',\n",
       " 'my',\n",
       " 'weren',\n",
       " 'if',\n",
       " 'him',\n",
       " \"needn't\",\n",
       " 'once',\n",
       " 'm',\n",
       " \"you'd\",\n",
       " 'this',\n",
       " 'what',\n",
       " 'at',\n",
       " 'it',\n",
       " 'its',\n",
       " \"haven't\",\n",
       " 'itself',\n",
       " 'couldn',\n",
       " 'theirs',\n",
       " 'is',\n",
       " 'until',\n",
       " 'against',\n",
       " 'now',\n",
       " 'mightn',\n",
       " 'too',\n",
       " 'but',\n",
       " 'into',\n",
       " 'will',\n",
       " 'other']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(set(stopwords.words(\"english\")))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dffb8916-ac3f-4ca0-83c1-5c64467a8a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kids Rule for Back-to-School The purchasing power of kids is a big part of why the back-to-school season has become such a huge marketing phenomenon.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# рассмотрим пример с датасета AG News\n",
    "example_stopwords = ds[\"train\"][16]\n",
    "text_stopwords = example_stopwords[\"text\"]\n",
    "text_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82c090fa-40cb-4a05-9abb-1676d2ac7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функции стемминга и лемматизации с удалением стоп-слов после токенезации (на примере соответствующих ранее представленных функций)\n",
    "def remove_stopwords(list_text):\n",
    "    without_stopwords = []\n",
    "\n",
    "    for i in list_text:\n",
    "        if i not in stop_words:\n",
    "            without_stopwords.append(i)\n",
    "    return without_stopwords\n",
    "\n",
    "def stemming_stopwords(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление знаков препинания\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # Применение стемминга\n",
    "    # Алгоритм использует набор правил, которые последовательно применяются к слову, чтобы удалить суффиксы. Эти правила основаны на структуре слова и его окончаниях.\n",
    "    stemmer = nltk.PorterStemmer()  # инициализируем стеммер\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]  # перебираем токены и применяем алгоритм стемминга\n",
    "    \n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38d765cb-7456-42f2-bdab-b93d3101e2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Kids Rule for Back-to-School The purchasing power of kids is a big part of why the back-to-school season has become such a huge marketing phenomenon.\n",
      "\n",
      "Стемминг (без удаления): ['kid', 'rule', 'for', 'backtoschool', 'the', 'purchas', 'power', 'of', 'kid', 'is', 'a', 'big', 'part', 'of', 'whi', 'the', 'backtoschool', 'season', 'ha', 'becom', 'such', 'a', 'huge', 'market', 'phenomenon']\n",
      "\n",
      "Стемминг (с удалением): ['kid', 'rule', 'backtoschool', 'purchas', 'power', 'kid', 'big', 'part', 'backtoschool', 'season', 'becom', 'huge', 'market', 'phenomenon']\n"
     ]
    }
   ],
   "source": [
    "# сравним стемминг этого примера, когда не убираем стоп-слова и убираем\n",
    "print('Исходный текст:', text_stopwords)\n",
    "print()\n",
    "print('Стемминг (без удаления):', stemming(text_stopwords))\n",
    "print()\n",
    "print('Стемминг (с удалением):', stemming_stopwords(text_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe2fbab9-6899-4a7f-b555-820fe02b7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# то же самое для лемматизации\n",
    "def lemmatization_stopwords(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление знаков препинания\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # Применение лемматизации\n",
    "    # WordNet - алгоритм ищет слово в базе данных WordNet и возвращает его базовую форму (лемму)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09acb38d-24c1-4b39-a4f7-4a578248d961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Kids Rule for Back-to-School The purchasing power of kids is a big part of why the back-to-school season has become such a huge marketing phenomenon.\n",
      "\n",
      "Лемматизация (без удаления): ['kid', 'rule', 'for', 'backtoschool', 'the', 'purchasing', 'power', 'of', 'kid', 'is', 'a', 'big', 'part', 'of', 'why', 'the', 'backtoschool', 'season', 'ha', 'become', 'such', 'a', 'huge', 'marketing', 'phenomenon']\n",
      "\n",
      "Стемминг (с удалением): ['kid', 'rule', 'backtoschool', 'purchasing', 'power', 'kid', 'big', 'part', 'backtoschool', 'season', 'become', 'huge', 'marketing', 'phenomenon']\n"
     ]
    }
   ],
   "source": [
    "# сравним лемматизацию этого примера, когда не убираем стоп-слова и убираем\n",
    "print('Исходный текст:', text_stopwords)\n",
    "print()\n",
    "print('Лемматизация (без удаления):', lemmatization(text_stopwords))\n",
    "print()\n",
    "print('Стемминг (с удалением):', lemmatization_stopwords(text_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50901aaf-170c-469a-a540-c8d91ccf6ab7",
   "metadata": {},
   "source": [
    "# Датасет IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48ddf0a9-214a-46b8-9747-16d13c08ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89a2d2a5-adb5-4fd0-b60f-93f3669fb162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example1 = dataset[\"train\"][0]['text']\n",
    "text_example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e53bd649-5a8a-468c-9662-1a4f61b41f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_example1 = dataset[\"train\"][0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af013f-ed5b-491a-bda5-0843e722b1a8",
   "metadata": {},
   "source": [
    "**Посмотрим на токенезацию**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "470a4235-d263-49f5-9b16-27312ee4f3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'rented', 'I', 'AM', 'CURIOUS-YELLOW', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967', '.', 'I', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'U.S.', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', ',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', '``', 'controversial', \"''\", 'I', 'really', 'had', 'to', 'see', 'this', 'for', 'myself.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'plot', 'is', 'centered', 'around', 'a', 'young', 'Swedish', 'drama', 'student', 'named', 'Lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', '.', 'In', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attentions', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'Swede', 'thought', 'about', 'certain', 'political', 'issues', 'such', 'as', 'the', 'Vietnam', 'War', 'and', 'race', 'issues', 'in', 'the', 'United', 'States', '.', 'In', 'between', 'asking', 'politicians', 'and', 'ordinary', 'denizens', 'of', 'Stockholm', 'about', 'their', 'opinions', 'on', 'politics', ',', 'she', 'has', 'sex', 'with', 'her', 'drama', 'teacher', ',', 'classmates', ',', 'and', 'married', 'men.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'What', 'kills', 'me', 'about', 'I', 'AM', 'CURIOUS-YELLOW', 'is', 'that', '40', 'years', 'ago', ',', 'this', 'was', 'considered', 'pornographic', '.', 'Really', ',', 'the', 'sex', 'and', 'nudity', 'scenes', 'are', 'few', 'and', 'far', 'between', ',', 'even', 'then', 'it', \"'s\", 'not', 'shot', 'like', 'some', 'cheaply', 'made', 'porno', '.', 'While', 'my', 'countrymen', 'mind', 'find', 'it', 'shocking', ',', 'in', 'reality', 'sex', 'and', 'nudity', 'are', 'a', 'major', 'staple', 'in', 'Swedish', 'cinema', '.', 'Even', 'Ingmar', 'Bergman', ',', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'John', 'Ford', ',', 'had', 'sex', 'scenes', 'in', 'his', 'films.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'do', 'commend', 'the', 'filmmakers', 'for', 'the', 'fact', 'that', 'any', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purposes', 'rather', 'than', 'just', 'to', 'shock', 'people', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornographic', 'theaters', 'in', 'America', '.', 'I', 'AM', 'CURIOUS-YELLOW', 'is', 'a', 'good', 'film', 'for', 'anyone', 'wanting', 'to', 'study', 'the', 'meat', 'and', 'potatoes', '(', 'no', 'pun', 'intended', ')', 'of', 'Swedish', 'cinema', '.', 'But', 'really', ',', 'this', 'film', 'does', \"n't\", 'have', 'much', 'of', 'a', 'plot', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_imdb = nltk.word_tokenize(text_example1)\n",
    "print(tokens_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78af415a-de46-438c-8921-2aec3f639412",
   "metadata": {},
   "source": [
    "**Сравним результаты стемминга и лемматизации для текстов из imdb (соответствующие функции определены)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e353a582-15a7-493d-b05c-42bd4c48e235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "\n",
      "Стемминг: ['i', 'rent', 'i', 'am', 'curiousyellow', 'from', 'my', 'video', 'store', 'becaus', 'of', 'all', 'the', 'controversi', 'that', 'surround', 'it', 'when', 'it', 'wa', 'first', 'releas', 'in', '1967', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'wa', 'seiz', 'by', 'us', 'custom', 'if', 'it', 'ever', 'tri', 'to', 'enter', 'thi', 'countri', 'therefor', 'be', 'a', 'fan', 'of', 'film', 'consid', 'controversi', 'i', 'realli', 'had', 'to', 'see', 'thi', 'for', 'myselfbr', 'br', 'the', 'plot', 'is', 'center', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'name', 'lena', 'who', 'want', 'to', 'learn', 'everyth', 'she', 'can', 'about', 'life', 'in', 'particular', 'she', 'want', 'to', 'focu', 'her', 'attent', 'to', 'make', 'some', 'sort', 'of', 'documentari', 'on', 'what', 'the', 'averag', 'swede', 'thought', 'about', 'certain', 'polit', 'issu', 'such', 'as', 'the', 'vietnam', 'war', 'and', 'race', 'issu', 'in', 'the', 'unit', 'state', 'in', 'between', 'ask', 'politician', 'and', 'ordinari', 'denizen', 'of', 'stockholm', 'about', 'their', 'opinion', 'on', 'polit', 'she', 'ha', 'sex', 'with', 'her', 'drama', 'teacher', 'classmat', 'and', 'marri', 'menbr', 'br', 'what', 'kill', 'me', 'about', 'i', 'am', 'curiousyellow', 'is', 'that', '40', 'year', 'ago', 'thi', 'wa', 'consid', 'pornograph', 'realli', 'the', 'sex', 'and', 'nuditi', 'scene', 'are', 'few', 'and', 'far', 'between', 'even', 'then', 'it', 'not', 'shot', 'like', 'some', 'cheapli', 'made', 'porno', 'while', 'my', 'countrymen', 'mind', 'find', 'it', 'shock', 'in', 'realiti', 'sex', 'and', 'nuditi', 'are', 'a', 'major', 'stapl', 'in', 'swedish', 'cinema', 'even', 'ingmar', 'bergman', 'arguabl', 'their', 'answer', 'to', 'good', 'old', 'boy', 'john', 'ford', 'had', 'sex', 'scene', 'in', 'hi', 'filmsbr', 'br', 'i', 'do', 'commend', 'the', 'filmmak', 'for', 'the', 'fact', 'that', 'ani', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artist', 'purpos', 'rather', 'than', 'just', 'to', 'shock', 'peopl', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornograph', 'theater', 'in', 'america', 'i', 'am', 'curiousyellow', 'is', 'a', 'good', 'film', 'for', 'anyon', 'want', 'to', 'studi', 'the', 'meat', 'and', 'potato', 'no', 'pun', 'intend', 'of', 'swedish', 'cinema', 'but', 'realli', 'thi', 'film', 'doesnt', 'have', 'much', 'of', 'a', 'plot']\n",
      "\n",
      "Лемматизация: ['i', 'rented', 'i', 'am', 'curiousyellow', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'wa', 'first', 'released', 'in', '1967', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'wa', 'seized', 'by', 'u', 'custom', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', 'therefore', 'being', 'a', 'fan', 'of', 'film', 'considered', 'controversial', 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myselfbr', 'br', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'want', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', 'in', 'particular', 'she', 'want', 'to', 'focus', 'her', 'attention', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'swede', 'thought', 'about', 'certain', 'political', 'issue', 'such', 'a', 'the', 'vietnam', 'war', 'and', 'race', 'issue', 'in', 'the', 'united', 'state', 'in', 'between', 'asking', 'politician', 'and', 'ordinary', 'denizen', 'of', 'stockholm', 'about', 'their', 'opinion', 'on', 'politics', 'she', 'ha', 'sex', 'with', 'her', 'drama', 'teacher', 'classmate', 'and', 'married', 'menbr', 'br', 'what', 'kill', 'me', 'about', 'i', 'am', 'curiousyellow', 'is', 'that', '40', 'year', 'ago', 'this', 'wa', 'considered', 'pornographic', 'really', 'the', 'sex', 'and', 'nudity', 'scene', 'are', 'few', 'and', 'far', 'between', 'even', 'then', 'it', 'not', 'shot', 'like', 'some', 'cheaply', 'made', 'porno', 'while', 'my', 'countryman', 'mind', 'find', 'it', 'shocking', 'in', 'reality', 'sex', 'and', 'nudity', 'are', 'a', 'major', 'staple', 'in', 'swedish', 'cinema', 'even', 'ingmar', 'bergman', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'john', 'ford', 'had', 'sex', 'scene', 'in', 'his', 'filmsbr', 'br', 'i', 'do', 'commend', 'the', 'filmmaker', 'for', 'the', 'fact', 'that', 'any', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purpose', 'rather', 'than', 'just', 'to', 'shock', 'people', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornographic', 'theater', 'in', 'america', 'i', 'am', 'curiousyellow', 'is', 'a', 'good', 'film', 'for', 'anyone', 'wanting', 'to', 'study', 'the', 'meat', 'and', 'potato', 'no', 'pun', 'intended', 'of', 'swedish', 'cinema', 'but', 'really', 'this', 'film', 'doesnt', 'have', 'much', 'of', 'a', 'plot']\n"
     ]
    }
   ],
   "source": [
    "stemming_imdb1 = stemming(text_example1)\n",
    "lemma_imdb1 = lemmatization(text_example1)\n",
    "print('Исходный текст:', text_example1)\n",
    "print()\n",
    "print('Стемминг:', stemming_imdb1)\n",
    "print()\n",
    "print('Лемматизация:', lemma_imdb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22e83e36-9d13-44a3-9039-88c9670d4148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\'t go on to star in more and better films. Sadly, I didn\\'t think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat\\'s Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\\'s ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain\\'t no \"Paper Moon\" and only a very pale version of \"What\\'s Up, Doc\".'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ещё два примера\n",
    "text_example2 = dataset[\"train\"][10]['text']\n",
    "text_label2 = dataset[\"train\"][10]['label']\n",
    "text_example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16ddeba5-f4fb-468d-a95e-35448d6aa0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn't go on to star in more and better films. Sadly, I didn't think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat's Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich's ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain't no \"Paper Moon\" and only a very pale version of \"What's Up, Doc\".\n",
      "\n",
      "Стемминг: ['it', 'wa', 'great', 'to', 'see', 'some', 'of', 'my', 'favorit', 'star', 'of', '30', 'year', 'ago', 'includ', 'john', 'ritter', 'ben', 'gazarra', 'and', 'audrey', 'hepburn', 'they', 'look', 'quit', 'wonder', 'but', 'that', 'wa', 'it', 'they', 'were', 'not', 'given', 'ani', 'charact', 'or', 'good', 'line', 'to', 'work', 'with', 'i', 'neither', 'understood', 'or', 'care', 'what', 'the', 'charact', 'were', 'doingbr', 'br', 'some', 'of', 'the', 'smaller', 'femal', 'role', 'were', 'fine', 'patti', 'henson', 'and', 'colleen', 'camp', 'were', 'quit', 'compet', 'and', 'confid', 'in', 'their', 'small', 'sidekick', 'part', 'they', 'show', 'some', 'talent', 'and', 'it', 'is', 'sad', 'they', 'didnt', 'go', 'on', 'to', 'star', 'in', 'more', 'and', 'better', 'film', 'sadli', 'i', 'didnt', 'think', 'dorothi', 'stratten', 'got', 'a', 'chanc', 'to', 'act', 'in', 'thi', 'her', 'onli', 'import', 'film', 'rolebr', 'br', 'the', 'film', 'appear', 'to', 'have', 'some', 'fan', 'and', 'i', 'wa', 'veri', 'openmind', 'when', 'i', 'start', 'watch', 'it', 'i', 'am', 'a', 'big', 'peter', 'bogdanovich', 'fan', 'and', 'i', 'enjoy', 'hi', 'last', 'movi', 'cat', 'meow', 'and', 'all', 'hi', 'earli', 'one', 'from', 'target', 'to', 'nickleodeon', 'so', 'it', 'realli', 'surpris', 'me', 'that', 'i', 'wa', 'bare', 'abl', 'to', 'keep', 'awak', 'watch', 'thi', 'onebr', 'br', 'it', 'is', 'iron', 'that', 'thi', 'movi', 'is', 'about', 'a', 'detect', 'agenc', 'where', 'the', 'detect', 'and', 'client', 'get', 'romant', 'involv', 'with', 'each', 'other', 'five', 'year', 'later', 'bogdanovich', 'exgirlfriend', 'cybil', 'shepherd', 'had', 'a', 'hit', 'televis', 'seri', 'call', 'moonlight', 'steal', 'the', 'stori', 'idea', 'from', 'bogdanovich', 'of', 'cours', 'there', 'wa', 'a', 'great', 'differ', 'in', 'that', 'the', 'seri', 'reli', 'on', 'ton', 'of', 'witti', 'dialogu', 'while', 'thi', 'tri', 'to', 'make', 'do', 'with', 'slapstick', 'and', 'a', 'few', 'screwbal', 'linesbr', 'br', 'bottom', 'line', 'it', 'aint', 'no', 'paper', 'moon', 'and', 'onli', 'a', 'veri', 'pale', 'version', 'of', 'what', 'up', 'doc']\n",
      "\n",
      "Лемматизация: ['it', 'wa', 'great', 'to', 'see', 'some', 'of', 'my', 'favorite', 'star', 'of', '30', 'year', 'ago', 'including', 'john', 'ritter', 'ben', 'gazarra', 'and', 'audrey', 'hepburn', 'they', 'looked', 'quite', 'wonderful', 'but', 'that', 'wa', 'it', 'they', 'were', 'not', 'given', 'any', 'character', 'or', 'good', 'line', 'to', 'work', 'with', 'i', 'neither', 'understood', 'or', 'cared', 'what', 'the', 'character', 'were', 'doingbr', 'br', 'some', 'of', 'the', 'smaller', 'female', 'role', 'were', 'fine', 'patty', 'henson', 'and', 'colleen', 'camp', 'were', 'quite', 'competent', 'and', 'confident', 'in', 'their', 'small', 'sidekick', 'part', 'they', 'showed', 'some', 'talent', 'and', 'it', 'is', 'sad', 'they', 'didnt', 'go', 'on', 'to', 'star', 'in', 'more', 'and', 'better', 'film', 'sadly', 'i', 'didnt', 'think', 'dorothy', 'stratten', 'got', 'a', 'chance', 'to', 'act', 'in', 'this', 'her', 'only', 'important', 'film', 'rolebr', 'br', 'the', 'film', 'appears', 'to', 'have', 'some', 'fan', 'and', 'i', 'wa', 'very', 'openminded', 'when', 'i', 'started', 'watching', 'it', 'i', 'am', 'a', 'big', 'peter', 'bogdanovich', 'fan', 'and', 'i', 'enjoyed', 'his', 'last', 'movie', 'cat', 'meow', 'and', 'all', 'his', 'early', 'one', 'from', 'target', 'to', 'nickleodeon', 'so', 'it', 'really', 'surprised', 'me', 'that', 'i', 'wa', 'barely', 'able', 'to', 'keep', 'awake', 'watching', 'this', 'onebr', 'br', 'it', 'is', 'ironic', 'that', 'this', 'movie', 'is', 'about', 'a', 'detective', 'agency', 'where', 'the', 'detective', 'and', 'client', 'get', 'romantically', 'involved', 'with', 'each', 'other', 'five', 'year', 'later', 'bogdanovichs', 'exgirlfriend', 'cybil', 'shepherd', 'had', 'a', 'hit', 'television', 'series', 'called', 'moonlighting', 'stealing', 'the', 'story', 'idea', 'from', 'bogdanovich', 'of', 'course', 'there', 'wa', 'a', 'great', 'difference', 'in', 'that', 'the', 'series', 'relied', 'on', 'ton', 'of', 'witty', 'dialogue', 'while', 'this', 'try', 'to', 'make', 'do', 'with', 'slapstick', 'and', 'a', 'few', 'screwball', 'linesbr', 'br', 'bottom', 'line', 'it', 'aint', 'no', 'paper', 'moon', 'and', 'only', 'a', 'very', 'pale', 'version', 'of', 'whats', 'up', 'doc']\n"
     ]
    }
   ],
   "source": [
    "stemming_imdb2 = stemming(text_example2)\n",
    "lemma_imdb2 = lemmatization(text_example2)\n",
    "print('Исходный текст:', text_example2)\n",
    "print()\n",
    "print('Стемминг:', stemming_imdb2)\n",
    "print()\n",
    "print('Лемматизация:', lemma_imdb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0151f706-0535-45fb-ad17-8080bd953a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If the crew behind \"Zombie Chronicles\" ever read this, here\\'s some advice guys: <br /><br />1. In a \"Twist Ending\"-type movie, it\\'s not a good idea to insert close-ups of EVERY DEATH IN THE MOVIE in the opening credits. That tends to spoil the twists, y\\'know...? <br /><br />2. I know you produced this on a shoestring and - to be fair - you worked miracles with your budget but please, hire people who can actually act. Or at least, walk, talk and gesture at the same time. Joe Haggerty, I\\'m looking at you...<br /><br />3. If you\\'re going to set a part of your movie in the past, only do this if you have the props and costumes of the time.<br /><br />4. Twist endings are supposed to be a surprise. Sure, we don\\'t want twists that make no sense, but signposting the \"reveal\" as soon as you introduce a character? That\\'s not a great idea.<br /><br />Kudos to the guys for trying, but in all honesty, I\\'d rather they hadn\\'t...<br /><br />Only for zombie completists.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_example3 = dataset[\"train\"][20]['text']\n",
    "text_label3 = dataset[\"train\"][20]['label']\n",
    "text_example3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee8d6334-948e-4141-b1ce-13948edd2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: If the crew behind \"Zombie Chronicles\" ever read this, here's some advice guys: <br /><br />1. In a \"Twist Ending\"-type movie, it's not a good idea to insert close-ups of EVERY DEATH IN THE MOVIE in the opening credits. That tends to spoil the twists, y'know...? <br /><br />2. I know you produced this on a shoestring and - to be fair - you worked miracles with your budget but please, hire people who can actually act. Or at least, walk, talk and gesture at the same time. Joe Haggerty, I'm looking at you...<br /><br />3. If you're going to set a part of your movie in the past, only do this if you have the props and costumes of the time.<br /><br />4. Twist endings are supposed to be a surprise. Sure, we don't want twists that make no sense, but signposting the \"reveal\" as soon as you introduce a character? That's not a great idea.<br /><br />Kudos to the guys for trying, but in all honesty, I'd rather they hadn't...<br /><br />Only for zombie completists.\n",
      "\n",
      "Стемминг: ['if', 'the', 'crew', 'behind', 'zombi', 'chronicl', 'ever', 'read', 'thi', 'here', 'some', 'advic', 'guy', 'br', 'br', '1', 'in', 'a', 'twist', 'endingtyp', 'movi', 'it', 'not', 'a', 'good', 'idea', 'to', 'insert', 'closeup', 'of', 'everi', 'death', 'in', 'the', 'movi', 'in', 'the', 'open', 'credit', 'that', 'tend', 'to', 'spoil', 'the', 'twist', 'yknow', 'br', 'br', '2', 'i', 'know', 'you', 'produc', 'thi', 'on', 'a', 'shoestr', 'and', 'to', 'be', 'fair', 'you', 'work', 'miracl', 'with', 'your', 'budget', 'but', 'pleas', 'hire', 'peopl', 'who', 'can', 'actual', 'act', 'or', 'at', 'least', 'walk', 'talk', 'and', 'gestur', 'at', 'the', 'same', 'time', 'joe', 'haggerti', 'im', 'look', 'at', 'youbr', 'br', '3', 'if', 'your', 'go', 'to', 'set', 'a', 'part', 'of', 'your', 'movi', 'in', 'the', 'past', 'onli', 'do', 'thi', 'if', 'you', 'have', 'the', 'prop', 'and', 'costum', 'of', 'the', 'timebr', 'br', '4', 'twist', 'end', 'are', 'suppos', 'to', 'be', 'a', 'surpris', 'sure', 'we', 'dont', 'want', 'twist', 'that', 'make', 'no', 'sens', 'but', 'signpost', 'the', 'reveal', 'as', 'soon', 'as', 'you', 'introduc', 'a', 'charact', 'that', 'not', 'a', 'great', 'ideabr', 'br', 'kudo', 'to', 'the', 'guy', 'for', 'tri', 'but', 'in', 'all', 'honesti', 'id', 'rather', 'they', 'hadntbr', 'br', 'onli', 'for', 'zombi', 'completist']\n",
      "\n",
      "Лемматизация: ['if', 'the', 'crew', 'behind', 'zombie', 'chronicle', 'ever', 'read', 'this', 'here', 'some', 'advice', 'guy', 'br', 'br', '1', 'in', 'a', 'twist', 'endingtype', 'movie', 'it', 'not', 'a', 'good', 'idea', 'to', 'insert', 'closeup', 'of', 'every', 'death', 'in', 'the', 'movie', 'in', 'the', 'opening', 'credit', 'that', 'tends', 'to', 'spoil', 'the', 'twist', 'yknow', 'br', 'br', '2', 'i', 'know', 'you', 'produced', 'this', 'on', 'a', 'shoestring', 'and', 'to', 'be', 'fair', 'you', 'worked', 'miracle', 'with', 'your', 'budget', 'but', 'please', 'hire', 'people', 'who', 'can', 'actually', 'act', 'or', 'at', 'least', 'walk', 'talk', 'and', 'gesture', 'at', 'the', 'same', 'time', 'joe', 'haggerty', 'im', 'looking', 'at', 'youbr', 'br', '3', 'if', 'youre', 'going', 'to', 'set', 'a', 'part', 'of', 'your', 'movie', 'in', 'the', 'past', 'only', 'do', 'this', 'if', 'you', 'have', 'the', 'prop', 'and', 'costume', 'of', 'the', 'timebr', 'br', '4', 'twist', 'ending', 'are', 'supposed', 'to', 'be', 'a', 'surprise', 'sure', 'we', 'dont', 'want', 'twist', 'that', 'make', 'no', 'sense', 'but', 'signposting', 'the', 'reveal', 'a', 'soon', 'a', 'you', 'introduce', 'a', 'character', 'thats', 'not', 'a', 'great', 'ideabr', 'br', 'kudos', 'to', 'the', 'guy', 'for', 'trying', 'but', 'in', 'all', 'honesty', 'id', 'rather', 'they', 'hadntbr', 'br', 'only', 'for', 'zombie', 'completists']\n"
     ]
    }
   ],
   "source": [
    "stemming_imdb3 = stemming(text_example3)\n",
    "lemma_imdb3 = lemmatization(text_example3)\n",
    "print('Исходный текст:', text_example3)\n",
    "print()\n",
    "print('Стемминг:', stemming_imdb3)\n",
    "print()\n",
    "print('Лемматизация:', lemma_imdb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2c8bd-f59b-47ee-9062-20c845ccb00d",
   "metadata": {},
   "source": [
    "**2.** Показать конкретные случаи различия между стеммингом и лемматизацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f12f2a77-264b-4bf9-ad2b-7f2508602e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "\n",
      "Стемминг: ['i', 'rent', 'i', 'am', 'curiousyellow', 'from', 'my', 'video', 'store', 'becaus', 'of', 'all', 'the', 'controversi', 'that', 'surround', 'it', 'when', 'it', 'wa', 'first', 'releas', 'in', '1967', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'wa', 'seiz', 'by', 'us', 'custom', 'if', 'it', 'ever', 'tri', 'to', 'enter', 'thi', 'countri', 'therefor', 'be', 'a', 'fan', 'of', 'film', 'consid', 'controversi', 'i', 'realli', 'had', 'to', 'see', 'thi', 'for', 'myselfbr', 'br', 'the', 'plot', 'is', 'center', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'name', 'lena', 'who', 'want', 'to', 'learn', 'everyth', 'she', 'can', 'about', 'life', 'in', 'particular', 'she', 'want', 'to', 'focu', 'her', 'attent', 'to', 'make', 'some', 'sort', 'of', 'documentari', 'on', 'what', 'the', 'averag', 'swede', 'thought', 'about', 'certain', 'polit', 'issu', 'such', 'as', 'the', 'vietnam', 'war', 'and', 'race', 'issu', 'in', 'the', 'unit', 'state', 'in', 'between', 'ask', 'politician', 'and', 'ordinari', 'denizen', 'of', 'stockholm', 'about', 'their', 'opinion', 'on', 'polit', 'she', 'ha', 'sex', 'with', 'her', 'drama', 'teacher', 'classmat', 'and', 'marri', 'menbr', 'br', 'what', 'kill', 'me', 'about', 'i', 'am', 'curiousyellow', 'is', 'that', '40', 'year', 'ago', 'thi', 'wa', 'consid', 'pornograph', 'realli', 'the', 'sex', 'and', 'nuditi', 'scene', 'are', 'few', 'and', 'far', 'between', 'even', 'then', 'it', 'not', 'shot', 'like', 'some', 'cheapli', 'made', 'porno', 'while', 'my', 'countrymen', 'mind', 'find', 'it', 'shock', 'in', 'realiti', 'sex', 'and', 'nuditi', 'are', 'a', 'major', 'stapl', 'in', 'swedish', 'cinema', 'even', 'ingmar', 'bergman', 'arguabl', 'their', 'answer', 'to', 'good', 'old', 'boy', 'john', 'ford', 'had', 'sex', 'scene', 'in', 'hi', 'filmsbr', 'br', 'i', 'do', 'commend', 'the', 'filmmak', 'for', 'the', 'fact', 'that', 'ani', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artist', 'purpos', 'rather', 'than', 'just', 'to', 'shock', 'peopl', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornograph', 'theater', 'in', 'america', 'i', 'am', 'curiousyellow', 'is', 'a', 'good', 'film', 'for', 'anyon', 'want', 'to', 'studi', 'the', 'meat', 'and', 'potato', 'no', 'pun', 'intend', 'of', 'swedish', 'cinema', 'but', 'realli', 'thi', 'film', 'doesnt', 'have', 'much', 'of', 'a', 'plot']\n",
      "\n",
      "Лемматизация: ['i', 'rented', 'i', 'am', 'curiousyellow', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'wa', 'first', 'released', 'in', '1967', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'wa', 'seized', 'by', 'u', 'custom', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', 'therefore', 'being', 'a', 'fan', 'of', 'film', 'considered', 'controversial', 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myselfbr', 'br', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'want', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', 'in', 'particular', 'she', 'want', 'to', 'focus', 'her', 'attention', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'swede', 'thought', 'about', 'certain', 'political', 'issue', 'such', 'a', 'the', 'vietnam', 'war', 'and', 'race', 'issue', 'in', 'the', 'united', 'state', 'in', 'between', 'asking', 'politician', 'and', 'ordinary', 'denizen', 'of', 'stockholm', 'about', 'their', 'opinion', 'on', 'politics', 'she', 'ha', 'sex', 'with', 'her', 'drama', 'teacher', 'classmate', 'and', 'married', 'menbr', 'br', 'what', 'kill', 'me', 'about', 'i', 'am', 'curiousyellow', 'is', 'that', '40', 'year', 'ago', 'this', 'wa', 'considered', 'pornographic', 'really', 'the', 'sex', 'and', 'nudity', 'scene', 'are', 'few', 'and', 'far', 'between', 'even', 'then', 'it', 'not', 'shot', 'like', 'some', 'cheaply', 'made', 'porno', 'while', 'my', 'countryman', 'mind', 'find', 'it', 'shocking', 'in', 'reality', 'sex', 'and', 'nudity', 'are', 'a', 'major', 'staple', 'in', 'swedish', 'cinema', 'even', 'ingmar', 'bergman', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'john', 'ford', 'had', 'sex', 'scene', 'in', 'his', 'filmsbr', 'br', 'i', 'do', 'commend', 'the', 'filmmaker', 'for', 'the', 'fact', 'that', 'any', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purpose', 'rather', 'than', 'just', 'to', 'shock', 'people', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornographic', 'theater', 'in', 'america', 'i', 'am', 'curiousyellow', 'is', 'a', 'good', 'film', 'for', 'anyone', 'wanting', 'to', 'study', 'the', 'meat', 'and', 'potato', 'no', 'pun', 'intended', 'of', 'swedish', 'cinema', 'but', 'really', 'this', 'film', 'doesnt', 'have', 'much', 'of', 'a', 'plot']\n"
     ]
    }
   ],
   "source": [
    "# первый пример\n",
    "print('Исходный текст:', text_example1)\n",
    "print()\n",
    "print('Стемминг:', stemming_imdb1)\n",
    "print()\n",
    "print('Лемматизация:', lemma_imdb1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f863b93-cb26-4c04-ac29-a55477d5b7d7",
   "metadata": {},
   "source": [
    "1. was - wa - wa - в обоих примерах -s распознали как окончание слова (focus - focu - focus)\n",
    "2. U.S. - us - u - в обоих случаях смысл утерян\n",
    "3. controversial - controversi - controversial - при стемминге убрали суффикс, при лемматизации оставили часть речи\n",
    "4. everything - everyth - everything - при стемминге ошибочно удалилась часть слова, не суффикс -ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe5b4166-979b-4b01-bc80-1fa1996de224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: If the crew behind \"Zombie Chronicles\" ever read this, here's some advice guys: <br /><br />1. In a \"Twist Ending\"-type movie, it's not a good idea to insert close-ups of EVERY DEATH IN THE MOVIE in the opening credits. That tends to spoil the twists, y'know...? <br /><br />2. I know you produced this on a shoestring and - to be fair - you worked miracles with your budget but please, hire people who can actually act. Or at least, walk, talk and gesture at the same time. Joe Haggerty, I'm looking at you...<br /><br />3. If you're going to set a part of your movie in the past, only do this if you have the props and costumes of the time.<br /><br />4. Twist endings are supposed to be a surprise. Sure, we don't want twists that make no sense, but signposting the \"reveal\" as soon as you introduce a character? That's not a great idea.<br /><br />Kudos to the guys for trying, but in all honesty, I'd rather they hadn't...<br /><br />Only for zombie completists.\n",
      "\n",
      "Стемминг: ['if', 'the', 'crew', 'behind', 'zombi', 'chronicl', 'ever', 'read', 'thi', 'here', 'some', 'advic', 'guy', 'br', 'br', '1', 'in', 'a', 'twist', 'endingtyp', 'movi', 'it', 'not', 'a', 'good', 'idea', 'to', 'insert', 'closeup', 'of', 'everi', 'death', 'in', 'the', 'movi', 'in', 'the', 'open', 'credit', 'that', 'tend', 'to', 'spoil', 'the', 'twist', 'yknow', 'br', 'br', '2', 'i', 'know', 'you', 'produc', 'thi', 'on', 'a', 'shoestr', 'and', 'to', 'be', 'fair', 'you', 'work', 'miracl', 'with', 'your', 'budget', 'but', 'pleas', 'hire', 'peopl', 'who', 'can', 'actual', 'act', 'or', 'at', 'least', 'walk', 'talk', 'and', 'gestur', 'at', 'the', 'same', 'time', 'joe', 'haggerti', 'im', 'look', 'at', 'youbr', 'br', '3', 'if', 'your', 'go', 'to', 'set', 'a', 'part', 'of', 'your', 'movi', 'in', 'the', 'past', 'onli', 'do', 'thi', 'if', 'you', 'have', 'the', 'prop', 'and', 'costum', 'of', 'the', 'timebr', 'br', '4', 'twist', 'end', 'are', 'suppos', 'to', 'be', 'a', 'surpris', 'sure', 'we', 'dont', 'want', 'twist', 'that', 'make', 'no', 'sens', 'but', 'signpost', 'the', 'reveal', 'as', 'soon', 'as', 'you', 'introduc', 'a', 'charact', 'that', 'not', 'a', 'great', 'ideabr', 'br', 'kudo', 'to', 'the', 'guy', 'for', 'tri', 'but', 'in', 'all', 'honesti', 'id', 'rather', 'they', 'hadntbr', 'br', 'onli', 'for', 'zombi', 'completist']\n",
      "\n",
      "Лемматизация: ['if', 'the', 'crew', 'behind', 'zombie', 'chronicle', 'ever', 'read', 'this', 'here', 'some', 'advice', 'guy', 'br', 'br', '1', 'in', 'a', 'twist', 'endingtype', 'movie', 'it', 'not', 'a', 'good', 'idea', 'to', 'insert', 'closeup', 'of', 'every', 'death', 'in', 'the', 'movie', 'in', 'the', 'opening', 'credit', 'that', 'tends', 'to', 'spoil', 'the', 'twist', 'yknow', 'br', 'br', '2', 'i', 'know', 'you', 'produced', 'this', 'on', 'a', 'shoestring', 'and', 'to', 'be', 'fair', 'you', 'worked', 'miracle', 'with', 'your', 'budget', 'but', 'please', 'hire', 'people', 'who', 'can', 'actually', 'act', 'or', 'at', 'least', 'walk', 'talk', 'and', 'gesture', 'at', 'the', 'same', 'time', 'joe', 'haggerty', 'im', 'looking', 'at', 'youbr', 'br', '3', 'if', 'youre', 'going', 'to', 'set', 'a', 'part', 'of', 'your', 'movie', 'in', 'the', 'past', 'only', 'do', 'this', 'if', 'you', 'have', 'the', 'prop', 'and', 'costume', 'of', 'the', 'timebr', 'br', '4', 'twist', 'ending', 'are', 'supposed', 'to', 'be', 'a', 'surprise', 'sure', 'we', 'dont', 'want', 'twist', 'that', 'make', 'no', 'sense', 'but', 'signposting', 'the', 'reveal', 'a', 'soon', 'a', 'you', 'introduce', 'a', 'character', 'thats', 'not', 'a', 'great', 'ideabr', 'br', 'kudos', 'to', 'the', 'guy', 'for', 'trying', 'but', 'in', 'all', 'honesty', 'id', 'rather', 'they', 'hadntbr', 'br', 'only', 'for', 'zombie', 'completists']\n"
     ]
    }
   ],
   "source": [
    "# третий пример\n",
    "print('Исходный текст:', text_example3)\n",
    "print()\n",
    "print('Стемминг:', stemming_imdb3)\n",
    "print()\n",
    "print('Лемматизация:', lemma_imdb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b9b016-7384-4964-8909-08c8398f1ce1",
   "metadata": {},
   "source": [
    "1. this - thi - this\n",
    "2. Убранное -e/-ed: advice - advic - advice (но c some такого не произошло), movie - movi - movie, produced - produc - produced, miracle - miracl - miracle\n",
    "3. Замена -y на -i: every - everi - every\n",
    "4. Ошибочно убранное -ing: shoestring - shoestr - shoestring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a71335-a967-4669-8dfb-46db45d2d525",
   "metadata": {},
   "source": [
    "**Задание 1.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "640508da-4be6-4d4d-a797-55aee2b3e2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Terrible movie. Nuff Said.<br /><br />These Lines are Just Filler. The movie was bad. Why I have to expand on that I don't know. This is already a waste of my time. I just wanted to warn others. Avoid this movie. The acting sucks and the writing is just moronic. Bad in every way. The only nice thing about the movie are Deniz Akkaya's breasts. Even that was ruined though by a terrible and unneeded rape scene. The movie is a poorly contrived and totally unbelievable piece of garbage.<br /><br />OK now I am just going to rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste my time watching this offal. Then feeling compelled to warn others I create an account with IMDb only to discover that I have to write a friggen essay on the film just to express how bad I think it is. Totally unnecessary.\n",
      "\n",
      "Стандартная токенезация: ['Terrible', 'movie', '.', 'Nuff', 'Said.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'These', 'Lines', 'are', 'Just', 'Filler', '.', 'The', 'movie', 'was', 'bad', '.', 'Why', 'I', 'have', 'to', 'expand', 'on', 'that', 'I', 'do', \"n't\", 'know', '.', 'This', 'is', 'already', 'a', 'waste', 'of', 'my', 'time', '.', 'I', 'just', 'wanted', 'to', 'warn', 'others', '.', 'Avoid', 'this', 'movie', '.', 'The', 'acting', 'sucks', 'and', 'the', 'writing', 'is', 'just', 'moronic', '.', 'Bad', 'in', 'every', 'way', '.', 'The', 'only', 'nice', 'thing', 'about', 'the', 'movie', 'are', 'Deniz', 'Akkaya', \"'s\", 'breasts', '.', 'Even', 'that', 'was', 'ruined', 'though', 'by', 'a', 'terrible', 'and', 'unneeded', 'rape', 'scene', '.', 'The', 'movie', 'is', 'a', 'poorly', 'contrived', 'and', 'totally', 'unbelievable', 'piece', 'of', 'garbage.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'OK', 'now', 'I', 'am', 'just', 'going', 'to', 'rag', 'on', 'IMDb', 'for', 'this', 'stupid', 'rule', 'of', '10', 'lines', 'of', 'text', 'minimum', '.', 'First', 'I', 'waste', 'my', 'time', 'watching', 'this', 'offal', '.', 'Then', 'feeling', 'compelled', 'to', 'warn', 'others', 'I', 'create', 'an', 'account', 'with', 'IMDb', 'only', 'to', 'discover', 'that', 'I', 'have', 'to', 'write', 'a', 'friggen', 'essay', 'on', 'the', 'film', 'just', 'to', 'express', 'how', 'bad', 'I', 'think', 'it', 'is', '.', 'Totally', 'unnecessary', '.']\n"
     ]
    }
   ],
   "source": [
    "example_imdb = dataset[\"train\"][100]['text']\n",
    "\n",
    "token_imdb = nltk.word_tokenize(example_imdb)\n",
    "print('Исходный текст:', example_imdb)\n",
    "print()\n",
    "print('Стандартная токенезация:', token_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96ee3d90-95c0-432d-bb3d-0941ade4fb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стемминг: ['terribl', 'movi', 'nuff', 'saidbr', 'br', 'these', 'line', 'are', 'just', 'filler', 'the', 'movi', 'wa', 'bad', 'whi', 'i', 'have', 'to', 'expand', 'on', 'that', 'i', 'dont', 'know', 'thi', 'is', 'alreadi', 'a', 'wast', 'of', 'my', 'time', 'i', 'just', 'want', 'to', 'warn', 'other', 'avoid', 'thi', 'movi', 'the', 'act', 'suck', 'and', 'the', 'write', 'is', 'just', 'moron', 'bad', 'in', 'everi', 'way', 'the', 'onli', 'nice', 'thing', 'about', 'the', 'movi', 'are', 'deniz', 'akkaya', 'breast', 'even', 'that', 'wa', 'ruin', 'though', 'by', 'a', 'terribl', 'and', 'unneed', 'rape', 'scene', 'the', 'movi', 'is', 'a', 'poorli', 'contriv', 'and', 'total', 'unbeliev', 'piec', 'of', 'garbagebr', 'br', 'ok', 'now', 'i', 'am', 'just', 'go', 'to', 'rag', 'on', 'imdb', 'for', 'thi', 'stupid', 'rule', 'of', '10', 'line', 'of', 'text', 'minimum', 'first', 'i', 'wast', 'my', 'time', 'watch', 'thi', 'offal', 'then', 'feel', 'compel', 'to', 'warn', 'other', 'i', 'creat', 'an', 'account', 'with', 'imdb', 'onli', 'to', 'discov', 'that', 'i', 'have', 'to', 'write', 'a', 'friggen', 'essay', 'on', 'the', 'film', 'just', 'to', 'express', 'how', 'bad', 'i', 'think', 'it', 'is', 'total', 'unnecessari']\n",
      "\n",
      "Лемматизация: ['terrible', 'movie', 'nuff', 'saidbr', 'br', 'these', 'line', 'are', 'just', 'filler', 'the', 'movie', 'wa', 'bad', 'why', 'i', 'have', 'to', 'expand', 'on', 'that', 'i', 'dont', 'know', 'this', 'is', 'already', 'a', 'waste', 'of', 'my', 'time', 'i', 'just', 'wanted', 'to', 'warn', 'others', 'avoid', 'this', 'movie', 'the', 'acting', 'suck', 'and', 'the', 'writing', 'is', 'just', 'moronic', 'bad', 'in', 'every', 'way', 'the', 'only', 'nice', 'thing', 'about', 'the', 'movie', 'are', 'deniz', 'akkayas', 'breast', 'even', 'that', 'wa', 'ruined', 'though', 'by', 'a', 'terrible', 'and', 'unneeded', 'rape', 'scene', 'the', 'movie', 'is', 'a', 'poorly', 'contrived', 'and', 'totally', 'unbelievable', 'piece', 'of', 'garbagebr', 'br', 'ok', 'now', 'i', 'am', 'just', 'going', 'to', 'rag', 'on', 'imdb', 'for', 'this', 'stupid', 'rule', 'of', '10', 'line', 'of', 'text', 'minimum', 'first', 'i', 'waste', 'my', 'time', 'watching', 'this', 'offal', 'then', 'feeling', 'compelled', 'to', 'warn', 'others', 'i', 'create', 'an', 'account', 'with', 'imdb', 'only', 'to', 'discover', 'that', 'i', 'have', 'to', 'write', 'a', 'friggen', 'essay', 'on', 'the', 'film', 'just', 'to', 'express', 'how', 'bad', 'i', 'think', 'it', 'is', 'totally', 'unnecessary']\n"
     ]
    }
   ],
   "source": [
    "# предобработка через стемминг и лемматизацию (перед токенезацией удаляем знаки препинания)\n",
    "st_example_imdb = stemming(example_imdb)\n",
    "lemma_example_imdb = lemmatization(example_imdb)\n",
    "print('Стемминг:', st_example_imdb)\n",
    "print()\n",
    "print('Лемматизация:', lemma_example_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf707d0-b40c-4538-9edc-92fd25c58938",
   "metadata": {},
   "source": [
    "**При обработке первого датасета уже были написаны функции для удаления знаков препинания после токенезации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "828d4dba-dcde-4c72-86d8-d49deaf2e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удаление знаков препинания -> Токенезация\n",
      "Стемминг: ['terribl', 'movi', 'nuff', 'saidbr', 'br', 'these', 'line', 'are', 'just', 'filler', 'the', 'movi', 'wa', 'bad', 'whi', 'i', 'have', 'to', 'expand', 'on', 'that', 'i', 'dont', 'know', 'thi', 'is', 'alreadi', 'a', 'wast', 'of', 'my', 'time', 'i', 'just', 'want', 'to', 'warn', 'other', 'avoid', 'thi', 'movi', 'the', 'act', 'suck', 'and', 'the', 'write', 'is', 'just', 'moron', 'bad', 'in', 'everi', 'way', 'the', 'onli', 'nice', 'thing', 'about', 'the', 'movi', 'are', 'deniz', 'akkaya', 'breast', 'even', 'that', 'wa', 'ruin', 'though', 'by', 'a', 'terribl', 'and', 'unneed', 'rape', 'scene', 'the', 'movi', 'is', 'a', 'poorli', 'contriv', 'and', 'total', 'unbeliev', 'piec', 'of', 'garbagebr', 'br', 'ok', 'now', 'i', 'am', 'just', 'go', 'to', 'rag', 'on', 'imdb', 'for', 'thi', 'stupid', 'rule', 'of', '10', 'line', 'of', 'text', 'minimum', 'first', 'i', 'wast', 'my', 'time', 'watch', 'thi', 'offal', 'then', 'feel', 'compel', 'to', 'warn', 'other', 'i', 'creat', 'an', 'account', 'with', 'imdb', 'onli', 'to', 'discov', 'that', 'i', 'have', 'to', 'write', 'a', 'friggen', 'essay', 'on', 'the', 'film', 'just', 'to', 'express', 'how', 'bad', 'i', 'think', 'it', 'is', 'total', 'unnecessari']\n",
      "\n",
      "Токенезация -> Удаление знаков препинания\n",
      "Стемминг: ['terribl', 'movi', 'nuff', 'said.', 'br', 'br', 'these', 'line', 'are', 'just', 'filler', 'the', 'movi', 'wa', 'bad', 'whi', 'i', 'have', 'to', 'expand', 'on', 'that', 'i', 'do', \"n't\", 'know', 'thi', 'is', 'alreadi', 'a', 'wast', 'of', 'my', 'time', 'i', 'just', 'want', 'to', 'warn', 'other', 'avoid', 'thi', 'movi', 'the', 'act', 'suck', 'and', 'the', 'write', 'is', 'just', 'moron', 'bad', 'in', 'everi', 'way', 'the', 'onli', 'nice', 'thing', 'about', 'the', 'movi', 'are', 'deniz', 'akkaya', \"'s\", 'breast', 'even', 'that', 'wa', 'ruin', 'though', 'by', 'a', 'terribl', 'and', 'unneed', 'rape', 'scene', 'the', 'movi', 'is', 'a', 'poorli', 'contriv', 'and', 'total', 'unbeliev', 'piec', 'of', 'garbage.', 'br', 'br', 'ok', 'now', 'i', 'am', 'just', 'go', 'to', 'rag', 'on', 'imdb', 'for', 'thi', 'stupid', 'rule', 'of', '10', 'line', 'of', 'text', 'minimum', 'first', 'i', 'wast', 'my', 'time', 'watch', 'thi', 'offal', 'then', 'feel', 'compel', 'to', 'warn', 'other', 'i', 'creat', 'an', 'account', 'with', 'imdb', 'onli', 'to', 'discov', 'that', 'i', 'have', 'to', 'write', 'a', 'friggen', 'essay', 'on', 'the', 'film', 'just', 'to', 'express', 'how', 'bad', 'i', 'think', 'it', 'is', 'total', 'unnecessari']\n"
     ]
    }
   ],
   "source": [
    "print('Удаление знаков препинания -> Токенезация')\n",
    "print('Стемминг:', st_example_imdb)\n",
    "print()\n",
    "print('Токенезация -> Удаление знаков препинания')\n",
    "print('Стемминг:', steming_1(example_imdb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9924bc6-885c-4902-915e-3aecfbd87d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удаление знаков препинания -> Токенезация\n",
      "Лемматизация: ['terrible', 'movie', 'nuff', 'saidbr', 'br', 'these', 'line', 'are', 'just', 'filler', 'the', 'movie', 'wa', 'bad', 'why', 'i', 'have', 'to', 'expand', 'on', 'that', 'i', 'dont', 'know', 'this', 'is', 'already', 'a', 'waste', 'of', 'my', 'time', 'i', 'just', 'wanted', 'to', 'warn', 'others', 'avoid', 'this', 'movie', 'the', 'acting', 'suck', 'and', 'the', 'writing', 'is', 'just', 'moronic', 'bad', 'in', 'every', 'way', 'the', 'only', 'nice', 'thing', 'about', 'the', 'movie', 'are', 'deniz', 'akkayas', 'breast', 'even', 'that', 'wa', 'ruined', 'though', 'by', 'a', 'terrible', 'and', 'unneeded', 'rape', 'scene', 'the', 'movie', 'is', 'a', 'poorly', 'contrived', 'and', 'totally', 'unbelievable', 'piece', 'of', 'garbagebr', 'br', 'ok', 'now', 'i', 'am', 'just', 'going', 'to', 'rag', 'on', 'imdb', 'for', 'this', 'stupid', 'rule', 'of', '10', 'line', 'of', 'text', 'minimum', 'first', 'i', 'waste', 'my', 'time', 'watching', 'this', 'offal', 'then', 'feeling', 'compelled', 'to', 'warn', 'others', 'i', 'create', 'an', 'account', 'with', 'imdb', 'only', 'to', 'discover', 'that', 'i', 'have', 'to', 'write', 'a', 'friggen', 'essay', 'on', 'the', 'film', 'just', 'to', 'express', 'how', 'bad', 'i', 'think', 'it', 'is', 'totally', 'unnecessary']\n",
      "\n",
      "Токенезация -> Удаление знаков препинания\n",
      "Лемматизация: ['terrible', 'movie', 'nuff', 'said.', 'br', 'br', 'these', 'line', 'are', 'just', 'filler', 'the', 'movie', 'wa', 'bad', 'why', 'i', 'have', 'to', 'expand', 'on', 'that', 'i', 'do', \"n't\", 'know', 'this', 'is', 'already', 'a', 'waste', 'of', 'my', 'time', 'i', 'just', 'wanted', 'to', 'warn', 'others', 'avoid', 'this', 'movie', 'the', 'acting', 'suck', 'and', 'the', 'writing', 'is', 'just', 'moronic', 'bad', 'in', 'every', 'way', 'the', 'only', 'nice', 'thing', 'about', 'the', 'movie', 'are', 'deniz', 'akkaya', \"'s\", 'breast', 'even', 'that', 'wa', 'ruined', 'though', 'by', 'a', 'terrible', 'and', 'unneeded', 'rape', 'scene', 'the', 'movie', 'is', 'a', 'poorly', 'contrived', 'and', 'totally', 'unbelievable', 'piece', 'of', 'garbage.', 'br', 'br', 'ok', 'now', 'i', 'am', 'just', 'going', 'to', 'rag', 'on', 'imdb', 'for', 'this', 'stupid', 'rule', 'of', '10', 'line', 'of', 'text', 'minimum', 'first', 'i', 'waste', 'my', 'time', 'watching', 'this', 'offal', 'then', 'feeling', 'compelled', 'to', 'warn', 'others', 'i', 'create', 'an', 'account', 'with', 'imdb', 'only', 'to', 'discover', 'that', 'i', 'have', 'to', 'write', 'a', 'friggen', 'essay', 'on', 'the', 'film', 'just', 'to', 'express', 'how', 'bad', 'i', 'think', 'it', 'is', 'totally', 'unnecessary']\n"
     ]
    }
   ],
   "source": [
    "print('Удаление знаков препинания -> Токенезация')\n",
    "print('Лемматизация:', lemma_example_imdb)\n",
    "print()\n",
    "print('Токенезация -> Удаление знаков препинания')\n",
    "print('Лемматизация:', lemmatization_1(example_imdb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de9dd29-2bb7-4019-bb36-5aab77556e62",
   "metadata": {},
   "source": [
    "**Задание 1.5. На датасете imdb.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3522a95c-5741-46f5-8777-d40a7b2bef5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My interest in Dorothy Stratten caused me to purchase this video. Although it had great actors/actresses, there were just too many subplots going on to retain interest. Plus it just wasn't that interesting. Dialogue was stiff and confusing and the story just flipped around too much to be believable. I was pretty disappointed in what I believe was one of Audrey Hepburn's last movies. I'll always love John Ritter best in slapstick. He was just too pathetic here.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_imdb_stopwords = dataset[\"train\"][16]['text']\n",
    "text_imdb_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "015c06d4-7cc1-44fc-a07a-c78d4c632a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: My interest in Dorothy Stratten caused me to purchase this video. Although it had great actors/actresses, there were just too many subplots going on to retain interest. Plus it just wasn't that interesting. Dialogue was stiff and confusing and the story just flipped around too much to be believable. I was pretty disappointed in what I believe was one of Audrey Hepburn's last movies. I'll always love John Ritter best in slapstick. He was just too pathetic here.\n",
      "\n",
      "Стемминг (без удаления): ['my', 'interest', 'in', 'dorothi', 'stratten', 'caus', 'me', 'to', 'purchas', 'thi', 'video', 'although', 'it', 'had', 'great', 'actorsactress', 'there', 'were', 'just', 'too', 'mani', 'subplot', 'go', 'on', 'to', 'retain', 'interest', 'plu', 'it', 'just', 'wasnt', 'that', 'interest', 'dialogu', 'wa', 'stiff', 'and', 'confus', 'and', 'the', 'stori', 'just', 'flip', 'around', 'too', 'much', 'to', 'be', 'believ', 'i', 'wa', 'pretti', 'disappoint', 'in', 'what', 'i', 'believ', 'wa', 'one', 'of', 'audrey', 'hepburn', 'last', 'movi', 'ill', 'alway', 'love', 'john', 'ritter', 'best', 'in', 'slapstick', 'he', 'wa', 'just', 'too', 'pathet', 'here']\n",
      "\n",
      "Стемминг (с удалением): ['interest', 'dorothi', 'stratten', 'caus', 'purchas', 'video', 'although', 'great', 'actorsactress', 'mani', 'subplot', 'go', 'retain', 'interest', 'plu', 'wasnt', 'interest', 'dialogu', 'stiff', 'confus', 'stori', 'flip', 'around', 'much', 'believ', 'pretti', 'disappoint', 'believ', 'one', 'audrey', 'hepburn', 'last', 'movi', 'ill', 'alway', 'love', 'john', 'ritter', 'best', 'slapstick', 'pathet']\n"
     ]
    }
   ],
   "source": [
    "# сравним стемминг этого примера, когда не убираем стоп-слова и убираем\n",
    "print('Исходный текст:', text_imdb_stopwords)\n",
    "print()\n",
    "print('Стемминг (без удаления):', stemming(text_imdb_stopwords))\n",
    "print()\n",
    "print('Стемминг (с удалением):', stemming_stopwords(text_imdb_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "393b6e27-8187-4e14-85ea-dc6271d72a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: My interest in Dorothy Stratten caused me to purchase this video. Although it had great actors/actresses, there were just too many subplots going on to retain interest. Plus it just wasn't that interesting. Dialogue was stiff and confusing and the story just flipped around too much to be believable. I was pretty disappointed in what I believe was one of Audrey Hepburn's last movies. I'll always love John Ritter best in slapstick. He was just too pathetic here.\n",
      "\n",
      "Лемматизация (без удаления): ['my', 'interest', 'in', 'dorothy', 'stratten', 'caused', 'me', 'to', 'purchase', 'this', 'video', 'although', 'it', 'had', 'great', 'actorsactresses', 'there', 'were', 'just', 'too', 'many', 'subplots', 'going', 'on', 'to', 'retain', 'interest', 'plus', 'it', 'just', 'wasnt', 'that', 'interesting', 'dialogue', 'wa', 'stiff', 'and', 'confusing', 'and', 'the', 'story', 'just', 'flipped', 'around', 'too', 'much', 'to', 'be', 'believable', 'i', 'wa', 'pretty', 'disappointed', 'in', 'what', 'i', 'believe', 'wa', 'one', 'of', 'audrey', 'hepburn', 'last', 'movie', 'ill', 'always', 'love', 'john', 'ritter', 'best', 'in', 'slapstick', 'he', 'wa', 'just', 'too', 'pathetic', 'here']\n",
      "\n",
      "Стемминг (с удалением): ['interest', 'dorothy', 'stratten', 'caused', 'purchase', 'video', 'although', 'great', 'actorsactresses', 'many', 'subplots', 'going', 'retain', 'interest', 'plus', 'wasnt', 'interesting', 'dialogue', 'stiff', 'confusing', 'story', 'flipped', 'around', 'much', 'believable', 'pretty', 'disappointed', 'believe', 'one', 'audrey', 'hepburn', 'last', 'movie', 'ill', 'always', 'love', 'john', 'ritter', 'best', 'slapstick', 'pathetic']\n"
     ]
    }
   ],
   "source": [
    "# сравним лемматизацию этого примера, когда не убираем стоп-слова и убираем\n",
    "print('Исходный текст:', text_imdb_stopwords)\n",
    "print()\n",
    "print('Лемматизация (без удаления):', lemmatization(text_imdb_stopwords))\n",
    "print()\n",
    "print('Стемминг (с удалением):', lemmatization_stopwords(text_imdb_stopwords))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
