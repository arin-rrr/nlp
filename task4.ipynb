{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC_m-OmzsdPI"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "count0, count1, count2, count3 = 0, 0, 0, 0\n",
        "dataset_short = []\n",
        "for i in range(len(dataset['train'])):\n",
        "  if dataset['train'][i]['label'] == 0 and count0 < 2000:\n",
        "    dataset_short.append({'news': dataset['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset['train'][i]['label'] == 1 and count1 < 2000:\n",
        "    dataset_short.append({'news': dataset['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "  elif dataset['train'][i]['label'] == 2 and count2 < 2000:\n",
        "    dataset_short.append({'news': dataset['train'][i]['text'], 'label': 2})\n",
        "    count2 += 1\n",
        "  elif dataset['train'][i]['label'] == 3 and count3 < 2000:\n",
        "    dataset_short.append({'news': dataset['train'][i]['text'], 'label': 3})\n",
        "    count3 += 1\n",
        "len(dataset_short)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg0YqV0Csrh-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(dataset_short)\n",
        "dataset = {'train': dataset_short[:6400], 'test': dataset_short[6400:]}\n",
        "\n",
        "# Выводим объём выборок\n",
        "print('Объём train:', len(dataset['train']))\n",
        "print('Объём test:', len(dataset['test']))\n",
        "\n",
        "train_texts_news = [s['news'] for s in dataset['train']]\n",
        "test_texts_news = [s['news'] for s in dataset['test']]\n",
        "\n",
        "train_labels_news = [s['label'] for s in dataset['train']]\n",
        "test_labels_news = [s['label'] for s in dataset['test']]\n",
        "\n",
        "# ВЫводим примеры\n",
        "print(train_texts_news[0], train_labels_news[0])\n",
        "print(test_texts_news[0], test_labels_news[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AEhdSOjsy-l"
      },
      "outputs": [],
      "source": [
        "dataset_imdb = load_dataset(\"imdb\")\n",
        "\n",
        "count0, count1 = 0, 0\n",
        "dataset_short_imdb = []\n",
        "\n",
        "for i in range(len(dataset_imdb['train'])):\n",
        "  if dataset_imdb['train'][i]['label'] == 0 and count0 < 4000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_imdb['train'][i]['label'] == 1 and count1 < 4000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "\n",
        "random.shuffle(dataset_short_imdb)\n",
        "dataset_imdb = {'train': dataset_short_imdb[:6400], 'test': dataset_short_imdb[6400:]}\n",
        "\n",
        "# Выводим объём выборок\n",
        "print('Объём train:', len(dataset_imdb['train']))\n",
        "print('Объём test:', len(dataset_imdb['test']))\n",
        "\n",
        "train_texts_imdb = [s['text'] for s in dataset_imdb['train']]\n",
        "test_texts_imdb = [s['text'] for s in dataset_imdb['test']]\n",
        "\n",
        "train_labels_imdb = [s['label'] for s in dataset_imdb['train']]\n",
        "test_labels_imdb = [s['label'] for s in dataset_imdb['test']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4GI-RMCs-DV"
      },
      "outputs": [],
      "source": [
        "print(train_texts_imdb[0])\n",
        "print(train_labels_imdb[0])\n",
        "print()\n",
        "print(test_texts_imdb[0])\n",
        "print(test_labels_imdb[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBECvMe_s_wv"
      },
      "outputs": [],
      "source": [
        "# news example\n",
        "news_example = train_texts_news[0]\n",
        "print(news_example)\n",
        "\n",
        "# imdb example\n",
        "imdb_example = train_texts_imdb[0]\n",
        "print(imdb_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH-5qdqCqcyh"
      },
      "source": [
        "# Способы построения словаря (feature space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CgoFVXLoUaL"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = list(set(stopwords.words(\"english\")))\n",
        "\n",
        "# stopword - удаляем ли стопслова и спецсимволы? - 0 (нет, оставляем), 1 (удаляем)\n",
        "\n",
        "def all_words(text, stopword):\n",
        "  text = text.lower()\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  if stopword == 1:\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [re.sub(r'[^\\w\\s]', '', word) for word in tokens]\n",
        "\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzsD_75GtDSV"
      },
      "outputs": [],
      "source": [
        "print(all_words(news_example, 0))\n",
        "print(all_words(imdb_example, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anO0OCh8tO4x"
      },
      "outputs": [],
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "def nouns(text, stopword):\n",
        "  tokens = all_words(text, stopword)\n",
        "  tagged_words = pos_tag(tokens)\n",
        "  filter_words = [word for word, tag in tagged_words if tag.startswith(\"NN\")]\n",
        "\n",
        "  if stopword == 1:\n",
        "    filter_words = [word for word in filter_words if word not in stop_words]\n",
        "    filter_words = [re.sub(r'[^\\w\\s]', '', word) for word in filter_words]\n",
        "\n",
        "  return filter_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSXV8PUdunih"
      },
      "outputs": [],
      "source": [
        "print(nouns(news_example, 1))\n",
        "print(nouns(imdb_example, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FCpnkjRu7C6"
      },
      "outputs": [],
      "source": [
        "def nouns_adj(text, stopword):\n",
        "  tokens = all_words(text, stopword)\n",
        "  tagged_words = pos_tag(tokens)\n",
        "  filter_words = [word for word, tag in tagged_words if tag.startswith(\"NN\") or tag.startswith(\"JJ\")]\n",
        "\n",
        "  if stopword == 1:\n",
        "    filter_words = [word for word in filter_words if word not in stop_words]\n",
        "    filter_words = [re.sub(r'[^\\w\\s]', '', word) for word in filter_words]\n",
        "\n",
        "  return filter_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuIRIw3avJ8M"
      },
      "outputs": [],
      "source": [
        "print(nouns_adj(news_example, 0))\n",
        "print(nouns_adj(imdb_example, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZx55WOkvQI8"
      },
      "outputs": [],
      "source": [
        "def nouns_adj_verb(text, stopword):\n",
        "  tokens = all_words(text, stopword)\n",
        "  tagged_words = pos_tag(tokens)\n",
        "  filter_words = [word for word, tag in tagged_words if tag.startswith(\"NN\") or tag.startswith(\"JJ\") or tag.startswith(\"V\")]\n",
        "\n",
        "  if stopword == 1:\n",
        "    filter_words = [word for word in filter_words if word not in stop_words]\n",
        "    filter_words = [re.sub(r'[^\\w\\s]', '', word) for word in filter_words]\n",
        "\n",
        "  return filter_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6iYNp3evb7j"
      },
      "outputs": [],
      "source": [
        "print(nouns_adj_verb(news_example, 1))\n",
        "print(nouns_adj_verb(imdb_example, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMndMKUUwc9p"
      },
      "source": [
        "# Варианты представления весов признаков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YywWtYKU9zzH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "\n",
        "# определим методы векторизации\n",
        "vectorizers = {\n",
        "    \"Binary min\": CountVectorizer(binary=True, min_df=10),\n",
        "    \"Binary max\": CountVectorizer(binary=True, max_df=0.9),\n",
        "    \"Binary min_max\": CountVectorizer(binary=True, min_df=10, max_df=0.9),\n",
        "    \"Binary nothing\": CountVectorizer(binary=True),\n",
        "    \"Tf min\": CountVectorizer(binary=False, min_df=10),\n",
        "    \"Tf max\": CountVectorizer(binary=False, max_df=0.9),\n",
        "    \"Tf min_max\": CountVectorizer(binary=False, min_df=10, max_df=0.9),\n",
        "    \"Tf nothing\": CountVectorizer(binary=False),\n",
        "    \"TfIdf min\": TfidfVectorizer(min_df=10),\n",
        "    \"TfIdf max\": TfidfVectorizer(max_df=0.9),\n",
        "    \"TfIdf min_max\": TfidfVectorizer(min_df=10, max_df=0.9),\n",
        "    \"TfIdf nothing\": TfidfVectorizer()\n",
        "}\n",
        "\n",
        "classifier = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=2000),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoBpxG30Br2g"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def train_and_test(train_texts, test_texts, train_labels, test_labels, dataset_name, feature_space, stopword):\n",
        "  results = []\n",
        "\n",
        "  for vec_name, vectorizer in vectorizers.items():\n",
        "    for clf_name, clf in classifier.items():\n",
        "      # Создаём pipeline\n",
        "      pipeline = Pipeline([\n",
        "        (\"vectorizer\", vectorizer),\n",
        "        (\"classifier\", clf)\n",
        "      ])\n",
        "\n",
        "      # Обучаем модель\n",
        "      pipeline.fit(train_texts, train_labels)\n",
        "\n",
        "      # Делаем предсказания\n",
        "      y_pred = pipeline.predict(test_texts)\n",
        "\n",
        "      # Считаем метрики\n",
        "      accuracy = accuracy_score(test_labels, y_pred)\n",
        "      f1 = f1_score(test_labels, y_pred, average=\"macro\" if dataset_name == \"news\" else \"binary\")\n",
        "\n",
        "      # Сохраняем результаты\n",
        "      results.append([dataset_name, vec_name, clf_name, accuracy, f1, feature_space, stopword])\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0hP4FNAvC5bl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "itog = []\n",
        "# теперь проводим эксперименты и сохраняем результаты\n",
        "for d in ['news', 'imdb']:\n",
        "  for feature_space in range(1, 5):\n",
        "    for stopword in [0, 1]:\n",
        "      if d == 'news':\n",
        "        train_texts = train_texts_news\n",
        "        test_texts = test_texts_news\n",
        "        train_labels = train_labels_news\n",
        "        test_labels = test_labels_news\n",
        "      else:\n",
        "        train_texts = train_texts_imdb\n",
        "        test_texts = test_texts_imdb\n",
        "        train_labels = train_labels_imdb\n",
        "        test_labels = test_labels_imdb\n",
        "\n",
        "      if feature_space == 1:\n",
        "        train_texts = [' '.join(all_words(text, stopword)) for text in train_texts]\n",
        "        test_texts = [' '.join(all_words(text, stopword)) for text in test_texts]\n",
        "        fs= \"all_words\"\n",
        "      elif feature_space == 2:\n",
        "        train_texts = [' '.join(nouns(text, stopword)) for text in train_texts]\n",
        "        test_texts = [' '.join(nouns(text, stopword)) for text in test_texts]\n",
        "        fs = \"nouns\"\n",
        "      elif feature_space == 3:\n",
        "        train_texts = [' '.join(nouns_adj(text, stopword)) for text in train_texts]\n",
        "        test_texts = [' '.join(nouns_adj(text, stopword)) for text in test_texts]\n",
        "        fs= \"nouns_adj\"\n",
        "      else:\n",
        "        train_texts = [' '.join(nouns_adj_verb(text, stopword)) for text in train_texts]\n",
        "        test_texts = [' '.join(nouns_adj_verb(text, stopword)) for text in test_texts]\n",
        "        fs= \"nouns_adj_verbs\"\n",
        "\n",
        "      if stopword == 0:\n",
        "        st= \"оставили\"\n",
        "      else:\n",
        "        st= \"удалили\"\n",
        "\n",
        "      res = train_and_test(train_texts, test_texts, train_labels, test_labels, d, fs, st)\n",
        "      for i in res:\n",
        "        itog.append(i)\n",
        "      print(itog)\n",
        "\n",
        "df_results = pd.DataFrame(itog, columns=[\"Dataset\", \"Vectorizer\", \"Classifier\", \"Accuracy\", \"F1 Score\", \"Feature space\", \"Stopwords\"])\n",
        "df_results.to_excel(\"results.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOLGfOQTYQgC"
      },
      "outputs": [],
      "source": [
        "itog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4Rk67yEYiUA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_results = pd.DataFrame(itog, columns=[\"Dataset\", \"Vectorizer\", \"Classifier\", \"Accuracy\", \"F1 Score\", \"Feature space\", \"Stopwords\"])\n",
        "df_results.to_excel(\"results.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-m3RDtocHH2"
      },
      "source": [
        "# LSA через TruncatedSVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROg2ynFNcuV1"
      },
      "source": [
        "Изменяем параметр k, уменьшая размерность матрицы признаков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPjiWsGfc27a"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def lsa_k(train_text, train_labels, test_text, test_labels, dataset_name, feature_space, stopword):\n",
        "  results = []\n",
        "\n",
        "  for k in [100, 500, 1000]:\n",
        "    for vec_name, vectorizer in vectorizers.items():\n",
        "      pipeline = Pipeline([\n",
        "        (\"vectorizer\", vectorizer),\n",
        "        (\"svd\", TruncatedSVD(n_components=k, random_state=42)),\n",
        "        (\"clf\", LogisticRegression(max_iter=2000, random_state=42))\n",
        "        ])\n",
        "\n",
        "      pipeline.fit(train_text, train_labels)\n",
        "      y_pred = pipeline.predict(test_text)\n",
        "      accuracy = accuracy_score(test_labels, y_pred)\n",
        "      f1 = f1_score(test_labels, y_pred, average=\"macro\")\n",
        "\n",
        "      results.append([k, dataset_name, vec_name, accuracy, f1, feature_space, stopword])\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOXQk7fffaoj"
      },
      "outputs": [],
      "source": [
        "itog_lsa = []\n",
        "# теперь проводим эксперименты и сохраняем результаты\n",
        "for d in ['news', 'imdb']:\n",
        "  for feature_space in range(1, 5):\n",
        "    for stopword in [0, 1]:\n",
        "      if d == 'news':\n",
        "        train_texts = train_texts_news\n",
        "        test_texts = test_texts_news\n",
        "        train_labels = train_labels_news\n",
        "        test_labels = test_labels_news\n",
        "      else:\n",
        "        train_texts = train_texts_imdb\n",
        "        test_texts = test_texts_imdb\n",
        "        train_labels = train_labels_imdb\n",
        "        test_labels = test_labels_imdb\n",
        "\n",
        "      if feature_space == 1:\n",
        "        train_texts = [' '.join(all_words(text, stopword)) for text in train_texts]\n",
        "        test_texts = [' '.join(all_words(text, stopword)) for text in test_texts]\n",
        "        fs= \"all_words\"\n",
        "      elif feature_space == 2:\n",
        "        train_texts = [' '.join(nouns(text, stopword)) for text in train_texts]\n",
        "        test_texts = [' '.join(nouns(text, stopword)) for text in test_texts]\n",
        "        fs = \"nouns\"\n",
        "      elif feature_space == 3:\n",
        "        train_texts = [' '.join(nouns_adj(text, stopword)) for text in train_texts]\n",
        "        test_texts = [' '.join(nouns_adj(text, stopword)) for text in test_texts]\n",
        "        fs= \"nouns_adj\"\n",
        "      else:\n",
        "        train_texts = [' '.join(nouns_adj_verb(text, stopword)) for text in train_texts]\n",
        "        test_texts = [' '.join(nouns_adj_verb(text, stopword)) for text in test_texts]\n",
        "        fs= \"nouns_adj_verbs\"\n",
        "\n",
        "      if stopword == 0:\n",
        "        st= \"оставили\"\n",
        "      else:\n",
        "        st= \"удалили\"\n",
        "\n",
        "      res = lsa_k(train_texts, train_labels, test_texts, test_labels, d, fs, st)\n",
        "      for i in res:\n",
        "        itog_lsa.append(i)\n",
        "      print(len(itog_lsa))\n",
        "\n",
        "df_results = pd.DataFrame(itog_lsa, columns=[\"k\", \"Dataset\", \"Vectorizer\", \"Accuracy\", \"F1 Score\", \"Feature space\", \"Stopwords\"])\n",
        "df_results.to_excel(\"results_lsa.xlsx\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}