{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq9afPLo9K5C"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset_news = load_dataset(\"ag_news\")\n",
        "dataset_imdb = load_dataset('imdb')\n",
        "print((dataset_imdb))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "count0, count1, count2, count3 = 0, 0, 0, 0\n",
        "dataset_short_news = []\n",
        "for i in range(len(dataset_news['train'])):\n",
        "  if dataset_news['train'][i]['label'] == 0 and count0 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 1 and count1 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 2 and count2 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 2})\n",
        "    count2 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 3 and count3 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 3})\n",
        "    count3 += 1\n",
        "\n",
        "random.shuffle(dataset_short_news)\n",
        "dataset_news = {'train': dataset_short_news[:6400], 'test': dataset_short_news[6400:]}"
      ],
      "metadata": {
        "id": "BD0xE14p9h3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts_news = [s['news'] for s in dataset_news['train']]\n",
        "test_texts_news = [s['news'] for s in dataset_news['test']]\n",
        "\n",
        "train_labels_news = [s['label'] for s in dataset_news['train']]\n",
        "test_labels_news = [s['label'] for s in dataset_news['test']]\n",
        "\n",
        "# Выводим примеры\n",
        "print(train_texts_news[0], train_labels_news[0])\n",
        "print(test_texts_news[0], test_labels_news[0])"
      ],
      "metadata": {
        "id": "0L0bryAY9t9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count0, count1 = 0, 0\n",
        "dataset_short_imdb = []\n",
        "\n",
        "for i in range(len(dataset_imdb['train'])):\n",
        "  if dataset_imdb['train'][i]['label'] == 0 and count0 < 3000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_imdb['train'][i]['label'] == 1 and count1 < 3000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "\n",
        "random.shuffle(dataset_short_imdb)\n",
        "dataset_imdb = {'train': dataset_short_imdb[:4800], 'test': dataset_short_imdb[4800:]}\n",
        "\n",
        "train_texts_imdb = [s['text'] for s in dataset_imdb['train']]\n",
        "test_texts_imdb = [s['text'] for s in dataset_imdb['test']]\n",
        "\n",
        "train_labels_imdb = [s['label'] for s in dataset_imdb['train']]\n",
        "test_labels_imdb = [s['label'] for s in dataset_imdb['test']]"
      ],
      "metadata": {
        "id": "VhgEeK4SA2te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = list(set(stopwords.words(\"english\")))\n",
        "\n",
        "def preprocess(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [re.sub(r'[^\\w\\s]', '', word) for word in tokens]\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "wPzkIEHE-DE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts_news = [preprocess(i) for i in train_texts_news]"
      ],
      "metadata": {
        "id": "XWMCYbQq_bkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts_imdb = [preprocess(i) for i in train_texts_imdb]"
      ],
      "metadata": {
        "id": "quKb8hCAAlNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучаем XGBoost - GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "mSSEPAbmCkLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "KqD9ZoK_CoEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', GradientBoostingClassifier())\n",
        "])"
      ],
      "metadata": {
        "id": "VxCcjpbvCsZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_GBM_news = {\n",
        "    'clf__max_depth': [3, 4, 5],\n",
        "    'clf__learning_rate': [0.01, 0.1, 1],\n",
        "    'clf__subsample': [0.7, 0.9, 1],\n",
        "    'clf__max_features':['sqrt', 0.5]\n",
        "}"
      ],
      "metadata": {
        "id": "GX5OZ4NFEDx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_news_GBM = []\n",
        "\n",
        "for m_d in param_grid_GBM_news['clf__max_depth']:\n",
        "  for lr in param_grid_GBM_news['clf__learning_rate']:\n",
        "    for sub in param_grid_GBM_news['clf__subsample']:\n",
        "      for m_f in param_grid_GBM_news['clf__max_features']:\n",
        "        pipeline.set_params(\n",
        "          clf__max_depth = m_d,\n",
        "          clf__learning_rate = lr,\n",
        "          clf__subsample = sub,\n",
        "          clf__max_features = m_f\n",
        "        )\n",
        "        pipeline.fit(train_texts_news, train_labels_news)\n",
        "        y_pred_news = pipeline.predict(test_texts_news)\n",
        "\n",
        "        f1 = f1_score(test_labels_news, y_pred_news, average='weighted')\n",
        "        current = [m_d, lr, sub, m_f, f1]\n",
        "        res_news_GBM.append(current)\n",
        "        if len(res_news_GBM) % 10 == 0:\n",
        "          print(len(res_news_GBM))"
      ],
      "metadata": {
        "id": "_l03IDyfE2a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(res_news_GBM, columns=['max_depth', 'learning_rate', 'subsample', 'max_features', 'f1-score'])\n",
        "df_results.to_excel(\"results_news_GBM.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "2v7yST-7KNEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_imdb = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', GradientBoostingClassifier())\n",
        "])"
      ],
      "metadata": {
        "id": "I-KEhtFCOYkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_GBM_imdb = {\n",
        "    'clf__max_depth': [3, 5, 7],\n",
        "    'clf__learning_rate': [0.01, 0.1, 1],\n",
        "    'clf__subsample': [0.6, 0.8, 0.9],\n",
        "    'clf__max_features':['sqrt', 0.1]\n",
        "}"
      ],
      "metadata": {
        "id": "4gx_ucdSN392"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_imdb_GBM = []\n",
        "\n",
        "for m_d in param_grid_GBM_imdb['clf__max_depth']:\n",
        "  for lr in param_grid_GBM_imdb['clf__learning_rate']:\n",
        "    for sub in param_grid_GBM_imdb['clf__subsample']:\n",
        "      for m_f in param_grid_GBM_imdb['clf__max_features']:\n",
        "        pipeline_imdb.set_params(\n",
        "          clf__max_depth = m_d,\n",
        "          clf__learning_rate = lr,\n",
        "          clf__subsample = sub,\n",
        "          clf__max_features = m_f\n",
        "        )\n",
        "        pipeline_imdb.fit(train_texts_imdb, train_labels_imdb)\n",
        "        y_pred_imdb = pipeline_imdb.predict(test_texts_imdb)\n",
        "\n",
        "        f1 = f1_score(test_labels_imdb, y_pred_imdb, average='weighted')\n",
        "        current = [m_d, lr, sub, m_f, f1]\n",
        "        res_imdb_GBM.append(current)\n",
        "        if len(res_imdb_GBM) % 10 == 0:\n",
        "          print(len(res_imdb_GBM))"
      ],
      "metadata": {
        "id": "1RcnqcnYNjXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(res_imdb_GBM, columns=['max_depth', 'learning_rate', 'subsample', 'max_features', 'f1-score'])\n",
        "df_results.to_excel(\"results_imdb_GBM.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "5mMpnYtCTIal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучаем AdaBoost"
      ],
      "metadata": {
        "id": "ESrU1-6GUXOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "G8P0A2_GUxv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_est = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "ada = AdaBoostClassifier(\n",
        "    estimator=base_est,\n",
        "    n_estimators=50,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 3. Создаем pipeline\n",
        "pipeline_news_ada = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', ada)\n",
        "])"
      ],
      "metadata": {
        "id": "sCpycjObUbTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_news_ada = {\n",
        "    'clf__estimator__max_depth': [1, 3, 2],\n",
        "    'clf__n_estimators': [50, 100, 200],\n",
        "    'clf__learning_rate': [0.01, 0.1, 1.0]\n",
        "}"
      ],
      "metadata": {
        "id": "SZhmDbcOVoL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_news_ada = []\n",
        "\n",
        "for m_d in param_news_ada['clf__estimator__max_depth']:\n",
        "  for est in param_news_ada['clf__n_estimators']:\n",
        "    for lr in param_news_ada['clf__learning_rate']:\n",
        "      pipeline_news_ada.set_params(\n",
        "          clf__estimator__max_depth = m_d,\n",
        "          clf__n_estimators = est,\n",
        "          clf__learning_rate = lr\n",
        "      )\n",
        "      pipeline_news_ada.fit(train_texts_news, train_labels_news)\n",
        "      y_pred_news = pipeline_news_ada.predict(test_texts_news)\n",
        "\n",
        "      f1 = f1_score(test_labels_news, y_pred_news, average='weighted')\n",
        "      current = [m_d, est, lr, f1]\n",
        "      res_news_ada.append(current)\n",
        "      if len(res_news_ada) % 10 == 0:\n",
        "        print(len(res_news_ada))"
      ],
      "metadata": {
        "id": "fgu2i_7EV3UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(res_news_ada, columns=['max_depth', 'n_estimators', 'learning_rate', 'f1-score'])\n",
        "df_results.to_excel(\"results_news_ada.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "9FF5jPwDabHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_est = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "ada = AdaBoostClassifier(\n",
        "    estimator=base_est,\n",
        "    n_estimators=50,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 3. Создаем pipeline\n",
        "pipeline_imdb_ada = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', ada)\n",
        "])"
      ],
      "metadata": {
        "id": "HwQKs_SbaeG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_imdb_ada = {\n",
        "    'clf__estimator__max_depth': [1, 3, 5],\n",
        "    'clf__n_estimators': [50, 200, 300],\n",
        "    'clf__learning_rate': [0.01, 0.1, 1.0]\n",
        "}"
      ],
      "metadata": {
        "id": "X66TGPUVcQFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_imdb_ada = []\n",
        "\n",
        "for m_d in param_imdb_ada['clf__estimator__max_depth']:\n",
        "  for est in param_imdb_ada['clf__n_estimators']:\n",
        "    for lr in param_imdb_ada['clf__learning_rate']:\n",
        "      pipeline_imdb_ada.set_params(\n",
        "          clf__estimator__max_depth = m_d,\n",
        "          clf__n_estimators = est,\n",
        "          clf__learning_rate = lr\n",
        "      )\n",
        "      pipeline_imdb_ada.fit(train_texts_imdb, train_labels_imdb)\n",
        "      y_pred_imdb = pipeline_imdb_ada.predict(test_texts_imdb)\n",
        "\n",
        "      f1 = f1_score(test_labels_imdb, y_pred_imdb, average='weighted')\n",
        "      current = [m_d, est, lr, f1]\n",
        "      res_imdb_ada.append(current)\n",
        "      if len(res_imdb_ada) % 10 == 0:\n",
        "        print(len(res_imdb_ada))"
      ],
      "metadata": {
        "id": "nN6YjhUocYYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(res_imdb_ada, columns=['max_depth', 'n_estimators', 'learning_rate', 'f1-score'])\n",
        "df_results.to_excel(\"results_imdb_ada.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "ZqjIwqIJpElv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучаем XGBoost"
      ],
      "metadata": {
        "id": "ksoVJkSsxoNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "mUiHDIToxsDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "pAAy-_pYxzh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_news_xgboost = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', XGBClassifier())\n",
        "])"
      ],
      "metadata": {
        "id": "qU31kyMYyC17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_news_xgboost = {\n",
        "    'clf__max_depth': [3, 5, 7],\n",
        "    'clf__colsample_bytree': [0.7, 1],\n",
        "    'clf__subsample': [0.7, 1],\n",
        "    'clf__reg_lambda': [0.1, 0.5, 1],\n",
        "    'clf__learning_rate': [0.01, 0.1, 0.2]\n",
        "}"
      ],
      "metadata": {
        "id": "Q7ZOuFpRyPok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_news_xg = []\n",
        "for md in param_news_xgboost['clf__max_depth']:\n",
        "  for col in param_news_xgboost['clf__colsample_bytree']:\n",
        "    for sub in param_news_xgboost['clf__subsample']:\n",
        "      for reg in param_news_xgboost['clf__reg_lambda']:\n",
        "        for lr in param_news_xgboost['clf__learning_rate']:\n",
        "          pipeline_news_xgboost.set_params(\n",
        "            clf__max_depth = md,\n",
        "            clf__colsample_bytree = col,\n",
        "            clf__subsample = sub,\n",
        "            clf__reg_lambda = reg,\n",
        "            clf__learning_rate = lr\n",
        "          )\n",
        "          pipeline_news_xgboost.fit(train_texts_news, train_labels_news)\n",
        "          y_pred_news = pipeline_news_xgboost.predict(test_texts_news)\n",
        "          f1 = f1_score(test_labels_news, y_pred_news, average='weighted')\n",
        "          current = [md, col, sub, reg, lr, f1]\n",
        "          res_news_xg.append(current)\n",
        "          if len(res_news_xg) % 10 == 0:\n",
        "            print(len(res_news_xg))"
      ],
      "metadata": {
        "id": "oLdBKPXKyzts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(res_news_xg, columns=['max_depth', 'colsample_bytree', 'subsample','reg_lambda','learning_rate', 'f1-score'])\n",
        "df_results.to_excel(\"results_news_xg.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "RA5-SEC9rhyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_imdb_xgboost = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', XGBClassifier())\n",
        "])"
      ],
      "metadata": {
        "id": "0S5-4iXDsViK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_imdb_xgboost = {\n",
        "    'clf__max_depth': [3, 5, 8],\n",
        "    'clf__colsample_bytree': [0.5, 1],\n",
        "    'clf__subsample': [0.6, 0.9],\n",
        "    'clf__reg_lambda': [0.1, 0.5, 1],\n",
        "    'clf__learning_rate': [0.01, 0.1, 0.2]\n",
        "}"
      ],
      "metadata": {
        "id": "VZ7VGrPBsajw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_imdb_xg = []\n",
        "\n",
        "for md in param_imdb_xgboost['clf__max_depth']:\n",
        "  for col in param_imdb_xgboost['clf__colsample_bytree']:\n",
        "    for sub in param_imdb_xgboost['clf__subsample']:\n",
        "      for reg in param_imdb_xgboost['clf__reg_lambda']:\n",
        "        for lr in param_imdb_xgboost['clf__learning_rate']:\n",
        "          pipeline_imdb_xgboost.set_params(\n",
        "            clf__max_depth = md,\n",
        "            clf__colsample_bytree = col,\n",
        "            clf__subsample = sub,\n",
        "            clf__reg_lambda = reg,\n",
        "            clf__learning_rate = lr\n",
        "          )\n",
        "          pipeline_imdb_xgboost.fit(train_texts_imdb, train_labels_imdb)\n",
        "          y_pred_imdb = pipeline_imdb_xgboost.predict(test_texts_imdb)\n",
        "          f1 = f1_score(test_labels_imdb, y_pred_imdb, average='weighted')\n",
        "          current = [md, col, sub, reg, lr, f1]\n",
        "          res_imdb_xg.append(current)\n",
        "          if len(res_imdb_xg) % 10 == 0:\n",
        "            print(len(res_imdb_xg))"
      ],
      "metadata": {
        "id": "DNbL4ONisO4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(res_imdb_xg, columns=['max_depth', 'colsample_bytree', 'subsample','reg_lambda','learning_rate', 'f1-score'])\n",
        "df_results.to_excel(\"results_imdb_xg.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "l_jcPYOz4VbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}