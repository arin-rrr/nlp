{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ5pk119qO6O"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxMyQYz3rtdM"
      },
      "outputs": [],
      "source": [
        "dataset_news = load_dataset(\"ag_news\")\n",
        "count0, count1, count2, count3 = 0, 0, 0, 0\n",
        "dataset_short = []\n",
        "for i in range(len(dataset_news['train'])):\n",
        "  if dataset_news['train'][i]['label'] == 0 and count0 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 1 and count1 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 2 and count2 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 2})\n",
        "    count2 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 3 and count3 < 3000:\n",
        "    dataset_short.append({'news': dataset_news['train'][i]['text'], 'label': 3})\n",
        "    count3 += 1\n",
        "len(dataset_short)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfrurwCQr9i_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(dataset_short)\n",
        "dataset_news = {'train': dataset_short[:9600], 'test': dataset_short[9600:]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8As3tSMkr-PX"
      },
      "outputs": [],
      "source": [
        "# Выводим объём выборок\n",
        "print('Объём train:', len(dataset_news['train']))\n",
        "print('Объём test:', len(dataset_news['test']))\n",
        "\n",
        "train_texts_news = [s['news'] for s in dataset_news['train']]\n",
        "test_texts_news = [s['news'] for s in dataset_news['test']]\n",
        "\n",
        "train_labels_news = [s['label'] for s in dataset_news['train']]\n",
        "test_labels_news = [s['label'] for s in dataset_news['test']]\n",
        "\n",
        "# ВЫводим примеры\n",
        "print(train_texts_news[0], train_labels_news[0])\n",
        "print(test_texts_news[0], test_labels_news[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKoMUAYOsGHn"
      },
      "outputs": [],
      "source": [
        "# второй датасет\n",
        "dataset_imdb = load_dataset(\"imdb\")\n",
        "\n",
        "count0, count1 = 0, 0\n",
        "dataset_short_imdb = []\n",
        "\n",
        "for i in range(len(dataset_imdb['train'])):\n",
        "  if dataset_imdb['train'][i]['label'] == 0 and count0 < 6000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_imdb['train'][i]['label'] == 1 and count1 < 6000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "len(dataset_short_imdb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE_rKO6TsJLp"
      },
      "outputs": [],
      "source": [
        "random.shuffle(dataset_short_imdb)\n",
        "dataset_imdb = {'train': dataset_short_imdb[:9600], 'test': dataset_short_imdb[9600:]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45OAOxUXsLU6"
      },
      "outputs": [],
      "source": [
        "# Выводим объём выборок\n",
        "print('Объём train:', len(dataset_imdb['train']))\n",
        "print('Объём test:', len(dataset_imdb['test']))\n",
        "\n",
        "train_texts_imdb = [s['text'] for s in dataset_imdb['train']]\n",
        "test_texts_imdb = [s['text'] for s in dataset_imdb['test']]\n",
        "\n",
        "train_labels_imdb = [s['label'] for s in dataset_imdb['train']]\n",
        "test_labels_imdb = [s['label'] for s in dataset_imdb['test']]\n",
        "\n",
        "print(train_texts_imdb[0])\n",
        "print(train_labels_imdb[0])\n",
        "print()\n",
        "print(test_texts_imdb[0])\n",
        "print(test_labels_imdb[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsOTAowMsNtm"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from sklearn.metrics import f1_score\n",
        "stop_words = list(set(stopwords.words(\"english\")))\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YflUjh0OsPsf"
      },
      "outputs": [],
      "source": [
        "def base_preprocessing(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Zа-яА-Я ]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvYj2UhEsRvK"
      },
      "outputs": [],
      "source": [
        "def stopwords_special(text):\n",
        "  tokens = base_preprocessing(text)\n",
        "  tokens = [word for word in tokens if word not in stop_words]  # убираем стоп-слова\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi-N20UtsTto"
      },
      "outputs": [],
      "source": [
        "def stemming(text):\n",
        "  tokens = stopwords_special(text)\n",
        "\n",
        "  stemmer = nltk.PorterStemmer()  # инициализируем стеммер\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens]  # перебираем токены и применяем алгоритм стемминга\n",
        "  return stemmed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3y2vfQhsVf2"
      },
      "outputs": [],
      "source": [
        "def lemmatisation(text):\n",
        "  tokens = stopwords_special(text)\n",
        "\n",
        "  lemma = nltk.WordNetLemmatizer()\n",
        "  lemma_tokens = [lemma.lemmatize(token) for token in tokens]\n",
        "  return lemma_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfDoeYDCseRd"
      },
      "outputs": [],
      "source": [
        "def vectorize(train_texts, test_texts, method, edit):\n",
        "  if method == \"count\":\n",
        "    vectorizer = CountVectorizer()\n",
        "  else:\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # edit = 1 (stopwords_specoal), 2 (stemming), 3 (lemmatisation)\n",
        "  if edit == 1:\n",
        "    train_texts = [' '.join(stopwords_special(text)) for text in train_texts]\n",
        "    test_texts = [' '.join(stopwords_special(text)) for text in test_texts]\n",
        "  elif edit == 2:\n",
        "    train_texts = [' '.join(stemming(text)) for text in train_texts]\n",
        "    test_texts = [' '.join(stemming(text)) for text in test_texts]\n",
        "  elif edit == 3:\n",
        "    train_texts = [' '.join(lemmatisation(text)) for text in train_texts]\n",
        "    test_texts = [' '.join(lemmatisation(text)) for text in test_texts]\n",
        "\n",
        "  train_text = vectorizer.fit_transform(train_texts)\n",
        "  test_text = vectorizer.transform(test_texts)\n",
        "\n",
        "  return train_text, test_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kza9TnCKtHgZ"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for sizes in [(20, 40,), (20, 20, 20,), (10, 10, 20, 20)]:\n",
        "  for d in ['news', 'imdb']:\n",
        "    for m in ['count', 'tfidf']:\n",
        "      for edit in [1, 2, 3]:\n",
        "        if d == 'news':\n",
        "          train_texts = train_texts_news\n",
        "          test_texts = test_texts_news\n",
        "          train_labels = train_labels_news\n",
        "          test_labels = test_labels_news\n",
        "        else:\n",
        "          train_texts = train_texts_imdb\n",
        "          test_texts = test_texts_imdb\n",
        "          train_labels = train_labels_imdb\n",
        "          test_labels = test_labels_imdb\n",
        "\n",
        "\n",
        "        train_text, test_text = vectorize(train_texts, test_texts, m, edit)\n",
        "\n",
        "        mlp_clf = MLPClassifier(hidden_layer_sizes=sizes)\n",
        "        mlp_clf.fit(train_text, train_labels)\n",
        "        mlp_pred = mlp_clf.predict(test_text)\n",
        "        if edit == 1:\n",
        "          results.append([sizes, d, m, 'stopwords_special', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "        elif edit == 2:\n",
        "          results.append([sizes, d, m, 'stemming', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "        elif edit == 3:\n",
        "          results.append([sizes, d, m, 'lemmatisation', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "\n",
        "        if len(results) % 10 == 0:\n",
        "          print(len(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsdnJSnkA1LU"
      },
      "outputs": [],
      "source": [
        "len(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkWm6RwrNDpJ"
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame(results, columns=[\"hidden_layer_sizes\", \"Dataset\", \"Vectorizer\", \"Preprocess\", \"F1-score\"])\n",
        "df_results.to_excel(\"results_mlp.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylC-Plr9NubF"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for sizes in [(20, 20, 30, 50)]:\n",
        "  for d in ['news', 'imdb']:\n",
        "    for m in ['count', 'tfidf']:\n",
        "      for edit in [1, 2, 3]:\n",
        "        if d == 'news':\n",
        "          train_texts = train_texts_news\n",
        "          test_texts = test_texts_news\n",
        "          train_labels = train_labels_news\n",
        "          test_labels = test_labels_news\n",
        "        else:\n",
        "          train_texts = train_texts_imdb\n",
        "          test_texts = test_texts_imdb\n",
        "          train_labels = train_labels_imdb\n",
        "          test_labels = test_labels_imdb\n",
        "\n",
        "\n",
        "        train_text, test_text = vectorize(train_texts, test_texts, m, edit)\n",
        "\n",
        "        mlp_clf = MLPClassifier(hidden_layer_sizes=sizes)\n",
        "        mlp_clf.fit(train_text, train_labels)\n",
        "        mlp_pred = mlp_clf.predict(test_text)\n",
        "        if edit == 1:\n",
        "          results.append([sizes, d, m, 'stopwords_special', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "        elif edit == 2:\n",
        "          results.append([sizes, d, m, 'stemming', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "        elif edit == 3:\n",
        "          results.append([sizes, d, m, 'lemmatisation', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "\n",
        "        if len(results) % 10 == 0:\n",
        "          print(len(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4djL6s3N4LU"
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame(results, columns=[\"hidden_layer_sizes\", \"Dataset\", \"Vectorizer\", \"Preprocess\", \"F1-score\"])\n",
        "df_results.to_excel(\"results_mlp1.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zfb2AzwIRhCN"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for sizes in [(50, 60, 70, 80), (80, 100, 100, 100, 100)]:\n",
        "  for d in ['news', 'imdb']:\n",
        "    for m in ['count', 'tfidf']:\n",
        "      for edit in [1, 2, 3]:\n",
        "        if d == 'news':\n",
        "          train_texts = train_texts_news\n",
        "          test_texts = test_texts_news\n",
        "          train_labels = train_labels_news\n",
        "          test_labels = test_labels_news\n",
        "        else:\n",
        "          train_texts = train_texts_imdb\n",
        "          test_texts = test_texts_imdb\n",
        "          train_labels = train_labels_imdb\n",
        "          test_labels = test_labels_imdb\n",
        "\n",
        "\n",
        "        train_text, test_text = vectorize(train_texts, test_texts, m, edit)\n",
        "\n",
        "        mlp_clf = MLPClassifier(hidden_layer_sizes=sizes)\n",
        "        mlp_clf.fit(train_text, train_labels)\n",
        "        mlp_pred = mlp_clf.predict(test_text)\n",
        "        if edit == 1:\n",
        "          results.append([sizes, d, m, 'stopwords_special', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "        elif edit == 2:\n",
        "          results.append([sizes, d, m, 'stemming', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "        elif edit == 3:\n",
        "          results.append([sizes, d, m, 'lemmatisation', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "\n",
        "        if len(results) % 10 == 0:\n",
        "          print(len(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-SOefuBg7Dz"
      },
      "outputs": [],
      "source": [
        "len(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxp_Fi83g-zm"
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame(results, columns=[\"hidden_layer_sizes\", \"Dataset\", \"Vectorizer\", \"Preprocess\", \"F1-score\"])\n",
        "df_results.to_excel(\"results_mlp2.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPumYJnAhSh2"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "results_lsa = []\n",
        "for k in [10, 50, 100]:\n",
        "  for sizes in [(20, 40,), (20, 20, 20,), (10, 10, 20, 20)]:\n",
        "    for d in ['news', 'imdb']:\n",
        "      for m in ['count', 'tfidf']:\n",
        "        for edit in [1, 2, 3]:\n",
        "          if d == 'news':\n",
        "            train_texts = train_texts_news\n",
        "            test_texts = test_texts_news\n",
        "            train_labels = train_labels_news\n",
        "            test_labels = test_labels_news\n",
        "          else:\n",
        "            train_texts = train_texts_imdb\n",
        "            test_texts = test_texts_imdb\n",
        "            train_labels = train_labels_imdb\n",
        "            test_labels = test_labels_imdb\n",
        "\n",
        "\n",
        "          train_text, test_text = vectorize(train_texts, test_texts, m, edit)\n",
        "\n",
        "          svd = TruncatedSVD(n_components=k, random_state=42)\n",
        "          lsa = make_pipeline(svd)\n",
        "          train_lsa = lsa.fit_transform(train_text)\n",
        "          test_lsa = lsa.transform(test_text)\n",
        "\n",
        "          train_lsa = np.clip(train_lsa, 0, None)\n",
        "          test_lsa = np.clip(test_lsa, 0, None)\n",
        "\n",
        "          mlp_clf = MLPClassifier(hidden_layer_sizes=sizes)\n",
        "          mlp_clf.fit(train_text, train_labels)\n",
        "          mlp_pred = mlp_clf.predict(test_text)\n",
        "          if edit == 1:\n",
        "            results_lsa.append([sizes, d, m, 'stopwords_special', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "          elif edit == 2:\n",
        "            results_lsa.append([sizes, d, m, 'stemming', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "          elif edit == 3:\n",
        "            results_lsa.append([sizes, d, m, 'lemmatisation', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "\n",
        "          if len(results_lsa) % 10 == 0:\n",
        "            print(len(results_lsa))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZzXB74996fp"
      },
      "outputs": [],
      "source": [
        "len(results_lsa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aeUiIaQ99r7"
      },
      "outputs": [],
      "source": [
        "results_lsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48BMyeio-SSo"
      },
      "outputs": [],
      "source": [
        "new_results = []\n",
        "for i in range(len(results_lsa)):\n",
        "  if 0 <= i <= 35:\n",
        "    new_results.append([10] + results_lsa[i])\n",
        "  elif 36 <= i <= 71:\n",
        "    new_results.append([50] + results_lsa[i])\n",
        "  elif 72 <= i <= 107:\n",
        "    new_results.append([100] + results_lsa[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIu_YWm2_9w_"
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame(new_results, columns=[\"k\", \"hidden_layer_sizes\", \"Dataset\", \"Vectorizer\", \"Preprocess\", \"F1-score\"])\n",
        "df_results.to_excel(\"results_mlp_lsa.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lLYHELCbBtMc"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "results_lsa = []\n",
        "for k in [10, 50, 100]:\n",
        "  for sizes in [(20, 20, 30, 50), (50, 60, 70, 80), (80, 100, 100, 100, 100)]:\n",
        "    for d in ['news', 'imdb']:\n",
        "      for m in ['count', 'tfidf']:\n",
        "        for edit in [1, 2, 3]:\n",
        "          if d == 'news':\n",
        "            train_texts = train_texts_news\n",
        "            test_texts = test_texts_news\n",
        "            train_labels = train_labels_news\n",
        "            test_labels = test_labels_news\n",
        "          else:\n",
        "            train_texts = train_texts_imdb\n",
        "            test_texts = test_texts_imdb\n",
        "            train_labels = train_labels_imdb\n",
        "            test_labels = test_labels_imdb\n",
        "\n",
        "\n",
        "          train_text, test_text = vectorize(train_texts, test_texts, m, edit)\n",
        "\n",
        "          svd = TruncatedSVD(n_components=k, random_state=42)\n",
        "          lsa = make_pipeline(svd)\n",
        "          train_lsa = lsa.fit_transform(train_text)\n",
        "          test_lsa = lsa.transform(test_text)\n",
        "\n",
        "          train_lsa = np.clip(train_lsa, 0, None)\n",
        "          test_lsa = np.clip(test_lsa, 0, None)\n",
        "\n",
        "          mlp_clf = MLPClassifier(hidden_layer_sizes=sizes)\n",
        "          mlp_clf.fit(train_text, train_labels)\n",
        "          mlp_pred = mlp_clf.predict(test_text)\n",
        "          if edit == 1:\n",
        "            results_lsa.append([sizes, d, m, 'stopwords_special', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "          elif edit == 2:\n",
        "            results_lsa.append([sizes, d, m, 'stemming', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "          elif edit == 3:\n",
        "            results_lsa.append([sizes, d, m, 'lemmatisation', f1_score(test_labels, mlp_pred, average='macro')])\n",
        "\n",
        "          if len(results_lsa) % 10 == 0:\n",
        "            print(len(results_lsa))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaWtE75Bo-vA"
      },
      "outputs": [],
      "source": [
        "new_results = []\n",
        "for i in range(len(results_lsa)):\n",
        "  if 0 <= i <= 35:\n",
        "    new_results.append([10] + results_lsa[i])\n",
        "  elif 36 <= i <= 71:\n",
        "    new_results.append([50] + results_lsa[i])\n",
        "  elif 72 <= i <= 107:\n",
        "    new_results.append([100] + results_lsa[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(new_results, columns=[\"k\", \"hidden_layer_sizes\", \"Dataset\", \"Vectorizer\", \"Preprocess\", \"F1-score\"])\n",
        "df_results.to_excel(\"results_mlp_lsa1.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "sLqQTvPpMMVP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}