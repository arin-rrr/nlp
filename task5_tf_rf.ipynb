{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Настройка гиперпараметров Tf-IDF Vectorizer"
      ],
      "metadata": {
        "id": "8tohYngHqUAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **stopwords** - удаление стоп-слов\n",
        "*   **analyzer** ('word', 'char') - \"единица для векторизации\"\n",
        "*  **lowercaser** (True, False) - переводим ли текст в нижний регистр\n",
        "*  **ngram_range** - диапазон n-грамм\n",
        "* **max_df** - верхний порог частотности\n",
        "* **min_df** - нижний порого частотности\n",
        "* **use_idf** - используем ли IDF\n",
        "* **norm** (None, 'l1', 'l2') - нормализация Tf-IDF вектора\n",
        "\n"
      ],
      "metadata": {
        "id": "wXQ0qz-oquvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Настройка гиперпараметров RandomForest"
      ],
      "metadata": {
        "id": "xJoXpm1qr3DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **n_estimators** - количество деревьев\n",
        "*   **max_depth** - глубина дерева\n",
        "* **min_samples_split** - минимальное число образцов\n",
        "* **max_samples_split** - максимальное число образцов\n",
        "* **criterion** ('gini', 'entropy') - функция, по которой алгоритм оценивает качество разбиения узлов в деревьях\n",
        "\n"
      ],
      "metadata": {
        "id": "Xr4PWvFzr9mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Подготавливаем датасет AG News"
      ],
      "metadata": {
        "id": "rCYPHz7ctWJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset_news = load_dataset(\"ag_news\")"
      ],
      "metadata": {
        "id": "C7H8fCXxtbqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "count0, count1, count2, count3 = 0, 0, 0, 0\n",
        "dataset_short_news = []\n",
        "for i in range(len(dataset_news['train'])):\n",
        "  if dataset_news['train'][i]['label'] == 0 and count0 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 1 and count1 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 2 and count2 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 2})\n",
        "    count2 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 3 and count3 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 3})\n",
        "    count3 += 1\n",
        "\n",
        "random.shuffle(dataset_short_news)\n",
        "dataset_news = {'train': dataset_short_news[:6400], 'test': dataset_short_news[6400:]}"
      ],
      "metadata": {
        "id": "d9QKpLOLto92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts_news = [s['news'] for s in dataset_news['train']]\n",
        "test_texts_news = [s['news'] for s in dataset_news['test']]\n",
        "\n",
        "train_labels_news = [s['label'] for s in dataset_news['train']]\n",
        "test_labels_news = [s['label'] for s in dataset_news['test']]\n",
        "\n",
        "# Выводим примеры\n",
        "print(train_texts_news[0], train_labels_news[0])\n",
        "print(test_texts_news[0], test_labels_news[0])"
      ],
      "metadata": {
        "id": "8eJd7K0PuEDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# из предобработки сделаю только токенезацию и удаление спец.символов\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "import re\n",
        "\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [re.sub(r'[^\\w\\s]', '', word) for word in tokens]\n",
        "\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "8tpc-g1kuR3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts_news = [preprocess(i) for i in train_texts_news]"
      ],
      "metadata": {
        "id": "L1YgGWE-vJWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Настройка гиперпараметров randomForest и Tf-IDF через GridSearchCV (полный перебор)"
      ],
      "metadata": {
        "id": "jrN0OtGivZc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "dVdIIYr7vmbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer()),\n",
        "    (\"clf\", RandomForestClassifier(random_state=42))\n",
        "])"
      ],
      "metadata": {
        "id": "tZIMksj_v1-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    \"tfidf__stop_words\": [\"english\"],\n",
        "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"tfidf__analyzer\": [\"word\"],\n",
        "    \"tfidf__lowercase\": [True],\n",
        "    \"tfidf__max_df\": [0.7,0.8, 0.95],\n",
        "    \"tfidf__min_df\": [1, 3],\n",
        "    'tfidf__use_idf': [True, False],\n",
        "    \"tfidf__norm\": [None, 'l1', 'l2'],\n",
        "\n",
        "    \"clf__n_estimators\": [100, 200, 250],\n",
        "    \"clf__max_depth\": [None, 30],\n",
        "    \"clf__min_samples_split\": [2, 5, 7],\n",
        "}"
      ],
      "metadata": {
        "id": "mYCFDhoDwQbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_news = []\n",
        "\n",
        "for s_w in param_grid[\"tfidf__stop_words\"]:\n",
        "  for ng in param_grid['tfidf__ngram_range']:\n",
        "    for analyz in param_grid['tfidf__analyzer']:\n",
        "      for low in param_grid['tfidf__lowercase']:\n",
        "        for max_df in param_grid['tfidf__max_df']:\n",
        "          for min_df in param_grid['tfidf__min_df']:\n",
        "            for use_idf in param_grid['tfidf__use_idf']:\n",
        "              for n in param_grid['tfidf__norm']:\n",
        "                for n_est in param_grid['clf__n_estimators']:\n",
        "                  for max_d in param_grid['clf__max_depth']:\n",
        "                    for min_s in param_grid['clf__min_samples_split']:\n",
        "                      pipeline.set_params(\n",
        "                          tfidf__stop_words = s_w,\n",
        "                          tfidf__ngram_range = ng,\n",
        "                          tfidf__analyzer = analyz,\n",
        "                          tfidf__lowercase = low,\n",
        "                          tfidf__max_df = max_df,\n",
        "                          tfidf__min_df = min_df,\n",
        "                          tfidf__use_idf = use_idf,\n",
        "                          tfidf__norm = n,\n",
        "                          clf__n_estimators = n_est,\n",
        "                          clf__max_depth = max_d,\n",
        "                          clf__min_samples_split = min_s\n",
        "                      )\n",
        "\n",
        "                      pipeline.fit(train_texts_news, train_labels_news)\n",
        "                      y_pred_news = pipeline.predict(test_texts_news)\n",
        "                      f1 = f1_score(test_labels_news, y_pred_news, average='weighted')\n",
        "\n",
        "                      curr_news = [s_w, ng, analyz, low, max_df, min_df, use_idf, n, n_est, max_d, min_s, f1]\n",
        "                      res_news.append(curr_news)\n",
        "                      if len(res_news) % 10 == 0:\n",
        "                        print(len(res_news))"
      ],
      "metadata": {
        "id": "kmfAvknThcSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(res_news, columns=[\"stopwords\", \"ngram_range\", \"analyzer\", \"lowercase\", \"max_df\", \"min_df\", \"use_idf\", \"norm\", \"n_estimator\", \"max_depth\", \"min_samples_split\", 'f1-score'])\n",
        "df_results.to_excel(\"results_news_tfidf_rf.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "q07ccSS_W0Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делаем то же самое для логистической регрессии"
      ],
      "metadata": {
        "id": "PbJ7CQn6YO1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_lr = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer()),\n",
        "    (\"clf\", LogisticRegression())\n",
        "])"
      ],
      "metadata": {
        "id": "h2hEwv0xrSWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_lr = {\n",
        "    \"tfidf__stop_words\": [\"english\"],\n",
        "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "    \"tfidf__analyzer\": [\"word\"],\n",
        "    \"tfidf__lowercase\": [True],\n",
        "    \"tfidf__max_df\": [0.7, 0.8, 0.95],\n",
        "    \"tfidf__min_df\": [1, 3],\n",
        "    'tfidf__use_idf': [True, False],\n",
        "    \"tfidf__norm\": [None, 'l1', 'l2'],\n",
        "\n",
        "    \"clf__penalty\": ['l1', 'l2'],\n",
        "    \"clf__C\": [0.01, 0.001, 0.0001, 1],  # сила регуляризации\n",
        "    \"clf__solver\": ['liblinear', 'saga'],\n",
        "    \"clf__class_weight\": ['balanced', None]\n",
        "}"
      ],
      "metadata": {
        "id": "-61nDqPArcXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_news_lr = []\n",
        "\n",
        "for sw in param_grid_lr['tfidf__stop_words']:\n",
        "  for ng in param_grid_lr['tfidf__ngram_range']:\n",
        "    for analyz in param_grid_lr['tfidf__analyzer']:\n",
        "      for low in param_grid_lr['tfidf__lowercase']:\n",
        "        for max_df in param_grid_lr['tfidf__max_df']:\n",
        "          for min_df in param_grid_lr['tfidf__min_df']:\n",
        "            for use_idf in param_grid_lr['tfidf__use_idf']:\n",
        "              for n in param_grid_lr['tfidf__norm']:\n",
        "                for pen in param_grid_lr['clf__penalty']:\n",
        "                  for c in param_grid_lr['clf__C']:\n",
        "                    for sol in param_grid_lr['clf__solver']:\n",
        "                      for cw in param_grid_lr['clf__class_weight']:\n",
        "                          pipeline_lr.set_params(\n",
        "                            tfidf__stop_words = sw,\n",
        "                            tfidf__ngram_range = ng,\n",
        "                            tfidf__analyzer = analyz,\n",
        "                            tfidf__lowercase = low,\n",
        "                            tfidf__max_df = max_df,\n",
        "                            tfidf__min_df = min_df,\n",
        "                            tfidf__use_idf = use_idf,\n",
        "                            tfidf__norm = n,\n",
        "                            clf__penalty = pen,\n",
        "                            clf__C = c,\n",
        "                            clf__solver = sol,\n",
        "                            clf__class_weight = cw\n",
        "                          )\n",
        "\n",
        "                          pipeline_lr.fit(train_texts_news, train_labels_news)\n",
        "                          y_pred_news = pipeline_lr.predict(test_texts_news)\n",
        "                          f1 = f1_score(test_labels_news, y_pred_news, average='weighted')\n",
        "\n",
        "                          curr_news = [sw, ng, analyz, low, max_df, min_df, use_idf, n, pen, c, sol, cw, f1]\n",
        "                          res_news_lr.append(curr_news)\n",
        "                          if len(res_news_lr) % 10 == 0:\n",
        "                            print(len(res_news_lr))"
      ],
      "metadata": {
        "id": "NvXWAYsotIyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(res_news_lr, columns=[\"stopwords\", \"ngram_range\", \"analyzer\", \"lowercase\", \"max_df\", \"min_df\", \"use_idf\", \"norm\", \"penalty\", \"C\", \"solver\", \"class_weight\", 'f1-score'])\n",
        "df_results.to_excel(\"results_news_tfidf_lr1.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "ltXQETAR3NAn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}