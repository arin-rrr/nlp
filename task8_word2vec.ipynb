{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Добавляем классификатор naive_bayes и подбираем alpha"
      ],
      "metadata": {
        "id": "15chxI9tP-AQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ag-news**"
      ],
      "metadata": {
        "id": "VfV0K9FxQQOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "WniqlnKHP9z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "P9qPGTRxPsTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.__version__"
      ],
      "metadata": {
        "id": "In6Keh_BPxiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv4Nd4Q3NuUB"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset_news0 = load_dataset(\"ag_news\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count0, count1, count2, count3 = 0, 0, 0, 0\n",
        "dataset_news = []\n",
        "for i in range(len(dataset_news0['train'])):\n",
        "  if dataset_news0['train'][i]['label'] == 0 and count0 < 2000:\n",
        "    dataset_news.append({'news': dataset_news0['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_news0['train'][i]['label'] == 1 and count1 < 2000:\n",
        "    dataset_news.append({'news': dataset_news0['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "  elif dataset_news0['train'][i]['label'] == 2 and count2 < 2000:\n",
        "    dataset_news.append({'news': dataset_news0['train'][i]['text'], 'label': 2})\n",
        "    count2 += 1\n",
        "  elif dataset_news0['train'][i]['label'] == 3 and count3 < 2000:\n",
        "    dataset_news.append({'news': dataset_news0['train'][i]['text'], 'label': 3})\n",
        "    count3 += 1\n",
        "len(dataset_news)"
      ],
      "metadata": {
        "id": "z1fWrq8VQgbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(dataset_news)"
      ],
      "metadata": {
        "id": "eiwkfzFPRG-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_news_texts = [i['news'] for i in dataset_news]\n",
        "dataset_news_labels = [i['label'] for i in dataset_news]"
      ],
      "metadata": {
        "id": "VYsQZ2Z2RLAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**imdb**"
      ],
      "metadata": {
        "id": "RMDO66euRNzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_imdb0 = load_dataset(\"imdb\")\n",
        "\n",
        "count0, count1 = 0, 0\n",
        "dataset_imdb = []\n",
        "\n",
        "for i in range(len(dataset_imdb0['train'])):\n",
        "  if dataset_imdb0['train'][i]['label'] == 0 and count0 < 4000:\n",
        "    dataset_imdb.append({'text': dataset_imdb0['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_imdb0['train'][i]['label'] == 1 and count1 < 4000:\n",
        "    dataset_imdb.append({'text': dataset_imdb0['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "len(dataset_imdb)"
      ],
      "metadata": {
        "id": "w2QAt7QBRP_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(dataset_imdb)"
      ],
      "metadata": {
        "id": "p3-rDXv4RXMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_imdb_texts = [i['text'] for i in dataset_imdb]\n",
        "dataset_imdb_labels = [i['label'] for i in dataset_imdb]"
      ],
      "metadata": {
        "id": "HyQVoJV1RYgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Базовая предобработка"
      ],
      "metadata": {
        "id": "aau-3hY2Rvqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "stop_words = list(set(stopwords.words(\"english\")))"
      ],
      "metadata": {
        "id": "L9Lh4lAmRz4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preproccesing(text):\n",
        "  text = text.lower()\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [word for word in tokens if not word in stop_words]\n",
        "  tokens = word_tokenize(re.sub(r'[^a-zA-Zа-яА-Я ]', '', ' '.join(tokens)))  # убираем спец символы, числа и знаки препинания\n",
        "  return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "yyngVi-LR8eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_news_texts = [preproccesing(i) for i in dataset_news_texts]\n",
        "dataset_imdb_texts = [preproccesing(i) for i in dataset_imdb_texts]"
      ],
      "metadata": {
        "id": "n7mk2d0ZSCPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Векторизация"
      ],
      "metadata": {
        "id": "B_r-X9jBTBl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "WIzY4Mh3TDw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение naive_bayes и подбор alpha"
      ],
      "metadata": {
        "id": "2IybeNcGUB5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, ComplementNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "i20HVBwWUKUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка моделей и параметров\n",
        "models = {\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'BernoulliNB': BernoulliNB(),\n",
        "    'ComplementNB': ComplementNB()\n",
        "}\n",
        "\n",
        "param_grid = {'clf__alpha': [0.01, 0.1, 0.5, 1.0]}\n",
        "\n",
        "vectorizers = {\n",
        "    'Tfidf': TfidfVectorizer()\n",
        "}\n",
        "\n",
        "# Цикл по датасетам\n",
        "for dataset_name in ['imdb', 'ag_news']:\n",
        "    print(f\"Датасет: {dataset_name}\")\n",
        "    X, y = (dataset_imdb_texts, dataset_imdb_labels) if dataset_name == 'imdb' else (dataset_news_texts, dataset_news_labels)\n",
        "\n",
        "    # Для бинарных классов используем f1, для многоклассовых f1_macro\n",
        "    scoring = 'f1' if dataset_name == 'imdb' else 'f1_macro'\n",
        "\n",
        "    for vec_name, vectorizer in vectorizers.items():\n",
        "        print(f\"Векторизация: {vec_name}\")\n",
        "        for model_name, model in models.items():\n",
        "            pipe = Pipeline([\n",
        "                ('vect', vectorizer),\n",
        "                ('clf', model)\n",
        "            ])\n",
        "\n",
        "            grid = GridSearchCV(pipe, param_grid=param_grid, scoring=scoring, n_jobs=-1)\n",
        "            grid.fit(X, y)\n",
        "            best_alpha = grid.best_params_['clf__alpha']\n",
        "            best_score = grid.best_score_\n",
        "\n",
        "            print(f\"{model_name}: Лучшая alpha = {best_alpha}, {scoring} = {best_score:.4f}\")\n",
        "            print()\n"
      ],
      "metadata": {
        "id": "9C282-uJUnKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Обучение Word2Vec на AG News"
      ],
      "metadata": {
        "id": "K0Xv-idhZ5-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets gensim"
      ],
      "metadata": {
        "id": "2Od6yhY1O2hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "import multiprocessing"
      ],
      "metadata": {
        "id": "MkXBKLymPC8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Подготавливаем именно для news**"
      ],
      "metadata": {
        "id": "tWlr-lYcSJu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_news_texts = [i.split() for i in dataset_news_texts]"
      ],
      "metadata": {
        "id": "5zoSA3AxRP82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_news_texts[0])"
      ],
      "metadata": {
        "id": "I3d31YedSN6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(\n",
        "    sentences=dataset_news_texts,  # Используем ваши предобработанные данные\n",
        "    vector_size=300,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    sg=1,\n",
        "    workers=multiprocessing.cpu_count(),\n",
        "    epochs=10,\n",
        "    negative=5,\n",
        "    sample=1e-3\n",
        ")"
      ],
      "metadata": {
        "id": "tjVI43xGSboQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Проверка качества модели**"
      ],
      "metadata": {
        "id": "QRkLsbq_SpyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 тематических слов\n",
        "test_words = [\"apple\",  \"technology\", \"market\", \"investment\", \"president\", \"law\", \"olympics\", \"covid\", \"hospital\", \"law\"]\n",
        "\n",
        "for word in test_words:\n",
        "    if word in model.wv:\n",
        "        print(f\"Top-5 похожих на {word}:\")\n",
        "        for similar, score in model.wv.most_similar(word, topn=5):\n",
        "            print(f\"  {similar} (score: {score:.2f})\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"Слово {word} отсутствует в словаре\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "4lOEjyTxStMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}