{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Получаем эмбеддинги из предобученных моделей (BERT, DistilBERT), после используем их для классификации с помощью LR, SVM**"
      ],
      "metadata": {
        "id": "v9KdTffgdAOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Установка библиотек - transformers, torch"
      ],
      "metadata": {
        "id": "KW86q2mBdsrU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yMrj2oOcy7o"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "rdvXkKLsenZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "Ndj4cE9HetIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Инициализация BERT и DistilBERT"
      ],
      "metadata": {
        "id": "hUYl9lWNhCX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загрузка модели и токенизатора BERT**"
      ],
      "metadata": {
        "id": "XQdBN8DQfS6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_bert = 'bert-base-uncased'\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(model_name_bert)\n",
        "model_bert = BertModel.from_pretrained(model_name_bert)"
      ],
      "metadata": {
        "id": "goey5EkgfQjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция получения эмбеддингов из BERT'а\n",
        "\n",
        "def get_bert_embeddings(texts):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        # Токенизация и преобразование в тензоры\n",
        "        inputs = tokenizer_bert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "        # Получение эмбеддингов\n",
        "        with torch.no_grad():\n",
        "            outputs = model_bert(**inputs)\n",
        "\n",
        "        # Берём эмбеддинг\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
        "        embeddings.append(cls_embedding)\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "xG6TNhtxfg0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загрузки модели и токенизатора DistilBERT**"
      ],
      "metadata": {
        "id": "oJeT44o2gJfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel"
      ],
      "metadata": {
        "id": "OS2Ex1ChggrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_distilbert = 'distilbert-base-uncased'\n",
        "tokenizer_distilbert = DistilBertTokenizer.from_pretrained(model_name_distilbert)\n",
        "model_distilbert = DistilBertModel.from_pretrained(model_name_distilbert)"
      ],
      "metadata": {
        "id": "THLFmvgUgQ2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для получения эмбеддингов DistilBERT\n",
        "\n",
        "def get_distilbert_embeddings(texts):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer_distilbert(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model_distilbert(**inputs)\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
        "        embeddings.append(cls_embedding)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "hJ4Pkd9YgvSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Загрузка датасетов"
      ],
      "metadata": {
        "id": "21xoMcVshZE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "A5UIWzqShfdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets"
      ],
      "metadata": {
        "id": "cw20zDxDh1aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_news = datasets.load_dataset(\"ag_news\")"
      ],
      "metadata": {
        "id": "LmIDMB8th7do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_imdb = datasets.load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "5tLM86Evh96Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "count0, count1, count2, count3 = 0, 0, 0, 0\n",
        "dataset_short_news = []\n",
        "for i in range(len(dataset_news['train'])):\n",
        "  if dataset_news['train'][i]['label'] == 0 and count0 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 1 and count1 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 2 and count2 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 2})\n",
        "    count2 += 1\n",
        "  elif dataset_news['train'][i]['label'] == 3 and count3 < 2000:\n",
        "    dataset_short_news.append({'news': dataset_news['train'][i]['text'], 'label': 3})\n",
        "    count3 += 1\n",
        "\n",
        "random.shuffle(dataset_short_news)\n",
        "dataset_news = {'train': dataset_short_news[:6400], 'test': dataset_short_news[6400:]}"
      ],
      "metadata": {
        "id": "UaVx_9Phi-9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_X = []\n",
        "for i in range(len(dataset_news['train'])):\n",
        "  news_X.append(dataset_news['train'][i]['news'])\n",
        "for i in range(len(dataset_news['test'])):\n",
        "  news_X.append(dataset_news['test'][i]['news'])"
      ],
      "metadata": {
        "id": "QfDc15HWjMmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_y = []\n",
        "for i in range(len(dataset_news['train'])):\n",
        "  news_y.append(dataset_news['train'][i]['label'])\n",
        "for i in range(len(dataset_news['test'])):\n",
        "  news_y.append(dataset_news['test'][i]['label'])"
      ],
      "metadata": {
        "id": "7g4DMH1WjPU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count0, count1 = 0, 0\n",
        "dataset_short_imdb = []\n",
        "\n",
        "for i in range(len(dataset_imdb['train'])):\n",
        "  if dataset_imdb['train'][i]['label'] == 0 and count0 < 4000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_imdb['train'][i]['label'] == 1 and count1 < 4000:\n",
        "    dataset_short_imdb.append({'text': dataset_imdb['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "\n",
        "random.shuffle(dataset_short_imdb)\n",
        "dataset_imdb = {'train': dataset_short_imdb[:6400], 'test': dataset_short_imdb[6400:]}"
      ],
      "metadata": {
        "id": "IOcTrgTDjSHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_X = []\n",
        "for i in range(len(dataset_imdb['train'])):\n",
        "  imdb_X.append(dataset_imdb['train'][i]['text'])\n",
        "for i in range(len(dataset_imdb['test'])):\n",
        "  imdb_X.append(dataset_imdb['test'][i]['text'])"
      ],
      "metadata": {
        "id": "omU2ARqbjUTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_y = []\n",
        "for i in range(len(dataset_imdb['train'])):\n",
        "  imdb_y.append(dataset_imdb['train'][i]['label'])\n",
        "for i in range(len(dataset_imdb['test'])):\n",
        "  imdb_y.append(dataset_imdb['test'][i]['label'])"
      ],
      "metadata": {
        "id": "ao0Z8yv-jV9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Получаем эмбеддинги для текстов"
      ],
      "metadata": {
        "id": "bHGvmxb-j8N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_embeddings_news = get_bert_embeddings(news_X)\n",
        "bert_embeddings_imdb = get_bert_embeddings(imdb_X)"
      ],
      "metadata": {
        "id": "oRQR64kbkD9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distil_embeddings_news = get_distilbert_embeddings(news_X)\n",
        "distil_embeddings_imdb = get_distilbert_embeddings(imdb_X)"
      ],
      "metadata": {
        "id": "Jw3QhgaLkxOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Обучаем LR на эмбеддингах"
      ],
      "metadata": {
        "id": "zVUXrahCiO0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "c6vxDDSUiUum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**news - LR**"
      ],
      "metadata": {
        "id": "ZQSpioa5isDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_bert_news, X_test_bert_news, y_train_news, y_test_news = train_test_split(bert_embeddings_news, news_y, test_size=0.2)\n",
        "X_train_distil_news, X_test_distil_news, _, _ = train_test_split(distil_embeddings_news, news_y, test_size=0.2)"
      ],
      "metadata": {
        "id": "MokN4L4Zil9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_news_bert = LogisticRegression()"
      ],
      "metadata": {
        "id": "AaUe3-MCinyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_news_distil = LogisticRegression()"
      ],
      "metadata": {
        "id": "uYO624fkjGW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_news_bert.fit(X_train_bert_news, y_train_news)\n",
        "y_pred_news_bert = lr_news_bert.predict(X_test_bert_news)\n",
        "print(\"LR + BERT:\\n\", classification_report(y_test_news, y_pred_news_bert))"
      ],
      "metadata": {
        "id": "979afZX1jg5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_news_distil.fit(X_train_distil_news, y_train_news)\n",
        "y_pred_news_distil = lr_news_distil.predict(X_test_distil_news)\n",
        "print(\"LR + DistilBERT:\\n\", classification_report(y_test_news, y_pred_news_distil))"
      ],
      "metadata": {
        "id": "1FEcc0-2j3IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**news - SVM**"
      ],
      "metadata": {
        "id": "Knh1DZjWkRx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "fSz3xn3gkKeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_news_bert = SVC()\n",
        "svm_news_distil = SVC()"
      ],
      "metadata": {
        "id": "HzXJLMPKknGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_news_bert.fit(X_train_bert_news, y_train_news)\n",
        "y_pred_news_bert_svm = svm_news_bert.predict(X_test_bert_news)\n",
        "print(\"SVM + BERT:\\n\", classification_report(y_test_news, y_pred_news_bert_svm))"
      ],
      "metadata": {
        "id": "gjcO0ZNoktMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_news_distil.fit(X_train_distil_news, y_train_news)\n",
        "y_pred_news_distil_svm = svm_news_distil.predict(X_test_distil_news)\n",
        "print(\"SVM + DistilBERT:\\n\", classification_report(y_test_news, y_pred_news_distil_svm))"
      ],
      "metadata": {
        "id": "IXWqmctak_81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**imdb - LR**"
      ],
      "metadata": {
        "id": "_LdthOp_ljRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_bert_imdb, X_test_bert_imdb, y_train_imdb, y_test_imdb = train_test_split(bert_embeddings_imdb, imdb_y, test_size=0.2)\n",
        "X_train_distil_imdb, X_test_distil_imdb, _, _ = train_test_split(distil_embeddings_imdb, imdb_y, test_size=0.2)"
      ],
      "metadata": {
        "id": "ZjZU0iPzmZdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_imdb_bert = LogisticRegression()\n",
        "lr_imdb_distil = LogisticRegression()"
      ],
      "metadata": {
        "id": "C46hqvpJlmKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_imdb_bert.fit(X_train_bert_imdb, y_train_imdb)\n",
        "y_pred_bert = lr_imdb_bert.predict(X_test_bert_imdb)\n",
        "print(\"LR + BERT:\\n\", classification_report(y_test_imdb, y_pred_bert))"
      ],
      "metadata": {
        "id": "TV-_S3ZbmI76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_imdb_distil.fit(X_train_distil_imdb, y_train_imdb)\n",
        "y_pred_distil = lr_imdb_distil.predict(X_test_distil_imdb)\n",
        "print(\"LR + DsitilBERT:\\n\", classification_report(y_test_imdb, y_pred_distil))"
      ],
      "metadata": {
        "id": "3HDAtzPqm4rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**imdb - SVM**"
      ],
      "metadata": {
        "id": "Bn0w_rMMnU0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_imdb_bert = SVC()\n",
        "svm_imdb_distil = SVC()"
      ],
      "metadata": {
        "id": "1iSKVZHbnYyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_imdb_bert.fit(X_train_bert_imdb, y_train_imdb)\n",
        "y_pred_bert = svm_imdb_bert.predict(X_test_bert_imdb)\n",
        "print(\"SVM + BERT:\\n\", classification_report(y_test_imdb, y_pred_bert))"
      ],
      "metadata": {
        "id": "yPLpYOFInm0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_imdb_distil.fit(X_train_distil_imdb, y_train_imdb)\n",
        "y_pred_distil = svm_imdb_distil.predict(X_test_distil_imdb)\n",
        "print(\"SVM + DistilBERT:\\n\", classification_report(y_test_imdb, y_pred_distil))"
      ],
      "metadata": {
        "id": "pwE2xyjOnv6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вывод"
      ],
      "metadata": {
        "id": "TtUJ0yEHoD2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лучшие результаты при классификации на BERT, на DistilBERT результаты хуже на обоих датасетах"
      ],
      "metadata": {
        "id": "VtZQHfUVoOuh"
      }
    }
  ]
}