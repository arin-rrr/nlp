{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REvCCTZgwEn_"
      },
      "source": [
        "# Стандартно подключаем датасеты AG-News и imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gjbMi2bv_gr"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset_news0 = load_dataset(\"ag_news\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2zYCZ6fwxzO"
      },
      "outputs": [],
      "source": [
        "count0, count1, count2, count3 = 0, 0, 0, 0\n",
        "dataset_news = []\n",
        "for i in range(len(dataset_news0['train'])):\n",
        "  if dataset_news0['train'][i]['label'] == 0 and count0 < 2000:\n",
        "    dataset_news.append({'news': dataset_news0['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_news0['train'][i]['label'] == 1 and count1 < 2000:\n",
        "    dataset_news.append({'news': dataset_news0['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "  elif dataset_news0['train'][i]['label'] == 2 and count2 < 2000:\n",
        "    dataset_news.append({'news': dataset_news0['train'][i]['text'], 'label': 2})\n",
        "    count2 += 1\n",
        "  elif dataset_news0['train'][i]['label'] == 3 and count3 < 2000:\n",
        "    dataset_news.append({'news': dataset_news0['train'][i]['text'], 'label': 3})\n",
        "    count3 += 1\n",
        "len(dataset_news)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMcE5_Y-xisO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(dataset_news)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITL-iicgxudN"
      },
      "outputs": [],
      "source": [
        "# так как потом мне нужно будет использовать и кросс-валидацию, и разделеине на train/test, просто поделю на тексты и метки\n",
        "dataset_news_texts = [i['news'] for i in dataset_news]\n",
        "dataset_news_labels = [i['label'] for i in dataset_news]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo2UNrEQygnX"
      },
      "outputs": [],
      "source": [
        "dataset_imdb0 = load_dataset(\"imdb\")\n",
        "\n",
        "count0, count1 = 0, 0\n",
        "dataset_imdb = []\n",
        "\n",
        "for i in range(len(dataset_imdb0['train'])):\n",
        "  if dataset_imdb0['train'][i]['label'] == 0 and count0 < 4000:\n",
        "    dataset_imdb.append({'text': dataset_imdb0['train'][i]['text'], 'label': 0})\n",
        "    count0 += 1\n",
        "  elif dataset_imdb0['train'][i]['label'] == 1 and count1 < 4000:\n",
        "    dataset_imdb.append({'text': dataset_imdb0['train'][i]['text'], 'label': 1})\n",
        "    count1 += 1\n",
        "len(dataset_imdb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFYivqosyz7g"
      },
      "outputs": [],
      "source": [
        "random.shuffle(dataset_imdb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0t6yHawy3iu"
      },
      "outputs": [],
      "source": [
        "dataset_imdb_texts = [i['text'] for i in dataset_imdb]\n",
        "dataset_imdb_labels = [i['label'] for i in dataset_imdb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGzY-CahzcI0"
      },
      "source": [
        "# Функции предобработки и стемминг/лемматизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffFJQBiFz31e"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "stop_words = list(set(stopwords.words(\"english\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc2p7BJ5zg5J"
      },
      "outputs": [],
      "source": [
        "def preproccesing(text):\n",
        "  text = text.lower()\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [word for word in tokens if not word in stop_words]\n",
        "  tokens = word_tokenize(re.sub(r'[^a-zA-Zа-яА-Я ]', '', ' '.join(tokens)))  # убираем спец символы, числа и знаки препинания\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGkruKfk7qWA"
      },
      "outputs": [],
      "source": [
        "# примеры базового preprocesing'а для обоих датасетов\n",
        "print(dataset_news_texts[0])\n",
        "print(preproccesing(dataset_news_texts[0]))\n",
        "print()\n",
        "print(dataset_imdb_texts[0])\n",
        "print(preproccesing(dataset_imdb_texts[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh8GMks78l_X"
      },
      "outputs": [],
      "source": [
        "# добавляем стемминг и лемматизацию\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def stemming(text):\n",
        "  tokens = preproccesing(text)  # базовый preproccesing\n",
        "  stemmer = nltk.PorterStemmer()\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens]  # перебираем токены и применяем алгоритм стемминга\n",
        "  return stemmed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0opex6wK9Bja"
      },
      "outputs": [],
      "source": [
        "# примеры preproccesing'а после стемминга\n",
        "print(dataset_news_texts[0])\n",
        "print(stemming(dataset_news_texts[0]))\n",
        "print()\n",
        "print(dataset_imdb_texts[0])\n",
        "print(stemming(dataset_imdb_texts[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc_7KeSD9NwQ"
      },
      "outputs": [],
      "source": [
        "def lemmatisation(text):\n",
        "  tokens = preproccesing(text)\n",
        "  lemma = nltk.WordNetLemmatizer()\n",
        "  lemma_tokens = [lemma.lemmatize(token) for token in tokens]\n",
        "  return lemma_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaCWAsAx9fRS"
      },
      "outputs": [],
      "source": [
        "# примеры preproccesing'а после лемматизации\n",
        "print(dataset_news_texts[0])\n",
        "print(lemmatisation(dataset_news_texts[0]))\n",
        "print()\n",
        "print(dataset_imdb_texts[0])\n",
        "print(lemmatisation(dataset_imdb_texts[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3khWFTC9z2L"
      },
      "source": [
        "# Каждому датасету создадим по 3 варианта: базовый preproccesing, стэмминг и лемматизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3qrllnb985W"
      },
      "outputs": [],
      "source": [
        "dataset_news_base = [' '.join(preproccesing(text)) for text in dataset_news_texts]\n",
        "dataset_news_stem = [' '.join(stemming(text)) for text in dataset_news_texts]\n",
        "dataset_news_lemma = [' '.join(lemmatisation(text)) for text in dataset_news_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYKlxWcwo_q4"
      },
      "outputs": [],
      "source": [
        "dataset_imdb_base = [' '.join(preproccesing(text)) for text in dataset_imdb_texts]\n",
        "dataset_imdb_stem = [' '.join(stemming(text)) for text in dataset_imdb_texts]\n",
        "dataset_imdb_lemma = [' '.join(lemmatisation(text)) for text in dataset_imdb_texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNNKPvgRkuIn"
      },
      "source": [
        "# Создадим два vectorizers: униграммы и униграммы+биграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNUl9LJDkODK"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kNR92LslFrK"
      },
      "outputs": [],
      "source": [
        "vect_uni = TfidfVectorizer(ngram_range=(1, 1))\n",
        "vect_bi = TfidfVectorizer(ngram_range=(1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тестово обучаем RandomForest"
      ],
      "metadata": {
        "id": "-e7iLKeetK7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)"
      ],
      "metadata": {
        "id": "NIbt0oZUtSY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "cv_strat_rf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "sgujcnott9s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 30],      # Максимальная глубина дерева\n",
        "    'min_samples_split': [2, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],        # Минимальное число образцов в листе\n",
        "    'class_weight': [None, 'balanced']    # Балансировка классов\n",
        "}"
      ],
      "metadata": {
        "id": "_TulZfl3utap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_rf = GridSearchCV(\n",
        "    rf,\n",
        "    param_grid_rf,\n",
        "    cv=cv_strat_rf,\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "P9oQ6Zrou9v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_datasets ={\n",
        "    'news_base': (dataset_news_base, dataset_news_labels),\n",
        "    'news_stem': (dataset_news_stem, dataset_news_labels),\n",
        "    'news_lemma': (dataset_news_lemma, dataset_news_labels),\n",
        "    'imdb_base': (dataset_imdb_base, dataset_imdb_labels),\n",
        "    'imdb_stem': (dataset_imdb_stem, dataset_imdb_labels),\n",
        "    'imdb_lemma': (dataset_imdb_lemma, dataset_imdb_labels)\n",
        "}"
      ],
      "metadata": {
        "id": "rQL9GxrCvLV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_rf = []\n",
        "\n",
        "# Перебор векторизаторов и датасетов\n",
        "for vect_name, vect in [('Unigram', vect_uni), ('Bigram', vect_bi)]:\n",
        "    for data_name, (data, labels) in all_datasets.items():\n",
        "        print(f\"Vectorizer: {vect_name}, dataset: {data_name}\")\n",
        "\n",
        "        # Векторизация (обучаем ТОЛЬКО на train данных в кросс-валидации)\n",
        "        X = vect.fit_transform(data)\n",
        "\n",
        "        # Поиск параметров\n",
        "        grid_search_rf.fit(X, labels)\n",
        "\n",
        "        # Лучшие параметры и метрики\n",
        "        best_params = grid_search_rf.best_params_\n",
        "        best_score = grid_search_rf.best_score_\n",
        "\n",
        "        print(f\"Best F1: {best_score:.4f}\")\n",
        "        print(f\"Best params: {best_params}\")\n",
        "        print()\n",
        "\n",
        "        # Сохраняем результаты\n",
        "        results_rf.append({\n",
        "            'Vectorizer': vect_name,\n",
        "            'Dataset': data_name,\n",
        "            'Best F1': best_score,\n",
        "            'Best params': best_params\n",
        "        })\n",
        "\n",
        "# Результаты в DataFrame\n",
        "results_df_rf = pd.DataFrame(results_rf)\n",
        "print(results_df_rf)"
      ],
      "metadata": {
        "id": "drF47NU2vO6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3DCpYyNlvx-"
      },
      "source": [
        "# Обучаем LinearSVC\n",
        "Сначала проведем обучение по стратифицированной кросс-валидации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTVOX63Ml0Zm"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "import pandas as pd\n",
        "lin_scv = LinearSVC(C=1.0, random_state=42)  # параметры для SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRch6NJDl6Bv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "cv_strat = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # параметры для кросс-валидации\n",
        "\n",
        "# определяем сетку параметров\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'class_weight': [None, 'balanced'],\n",
        "    'loss': ['hinge', 'squared_hinge'],\n",
        "    'penalty': ['l2', 'l1'],\n",
        "    'dual': [True, False],\n",
        "    'multi_class': ['ovr', 'crammer_singer'],\n",
        "    'max_iter': [1000, 2000]\n",
        "}\n",
        "\n",
        "# объединяем всё в GridSearchCV\n",
        "grid_search = GridSearchCV(lin_scv, param_grid, cv=cv_strat, scoring='f1_weighted', verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7vJCnRxrHkZ"
      },
      "outputs": [],
      "source": [
        "# перебор всех датасетов\n",
        "\n",
        "all_datasets ={\n",
        "    'news_base': (dataset_news_base, dataset_news_labels),\n",
        "    'news_stem': (dataset_news_stem, dataset_news_labels),\n",
        "    'news_lemma': (dataset_news_lemma, dataset_news_labels),\n",
        "    'imdb_base': (dataset_imdb_base, dataset_imdb_labels),\n",
        "    'imdb_stem': (dataset_imdb_stem, dataset_imdb_labels),\n",
        "    'imdb_lemma': (dataset_imdb_lemma, dataset_imdb_labels)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgsjgW50ozvM"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "# Перебор векторизаторов и датасетов\n",
        "for vect_name, vect in [('Unigram', vect_uni), ('Bigram', vect_bi)]:\n",
        "    for data_name, (data, labels) in all_datasets.items():\n",
        "        print(f\"Vectorizer: {vect_name}, dataset: {data_name}\")\n",
        "\n",
        "        # Векторизация (обучаем ТОЛЬКО на train данных в кросс-валидации)\n",
        "        X = vect.fit_transform(data)\n",
        "\n",
        "        # Поиск параметров\n",
        "        grid_search.fit(X, labels)\n",
        "\n",
        "        # Лучшие параметры и метрики\n",
        "        best_params = grid_search.best_params_\n",
        "        best_score = grid_search.best_score_\n",
        "\n",
        "        print(f\"Best F1: {best_score:.4f}\")\n",
        "        print(f\"Best params: {best_params}\")\n",
        "        print()\n",
        "\n",
        "        # Сохраняем результаты\n",
        "        results.append({\n",
        "            'Vectorizer': vect_name,\n",
        "            'Dataset': data_name,\n",
        "            'Best F1': best_score,\n",
        "            'Best params': best_params\n",
        "        })\n",
        "\n",
        "# Результаты в DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}